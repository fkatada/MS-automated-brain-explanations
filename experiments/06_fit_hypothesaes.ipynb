{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neuro import config\n",
    "import neuro.decoding\n",
    "from neuro.baselines.hypothesaes.quickstart import train_sae, interpret_sae, generate_hypotheses, evaluate_hypotheses\n",
    "from neuro.baselines.hypothesaes.embedding import get_local_embeddings\n",
    "\n",
    "INTERPRETER_MODEL = \"gpt-4.1\"\n",
    "# ANNOTATOR_MODEL = 'meta-llama/Meta-Llama-3-8B-Instruct' # not actually used\n",
    "EMBEDDER = \"nomic-ai/modernbert-embed-base\" # Huggingface model, will run locally\n",
    "TASK_SPECIFIC_INSTRUCTIONS = \"\"\"All of the texts are reviews of restaurants on Yelp.\n",
    "Features should describe a specific aspect of the review. For example:\n",
    "- \"mentions long wait times to receive service\"\n",
    "- \"praises how a dish was cooked, with phrases like 'perfect medium-rare'\\\"\"\"\"\n",
    "\n",
    "# paths\n",
    "OUT_DIR = os.path.expanduser('~/mntv1/deep-fMRI/qa/hypothesaes')\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.environ['EMB_CACHE_DIR'] = os.path.join(OUT_DIR, os.path.expanduser('~/emb_cache'))\n",
    "CACHE_NAME = f\"yelp_quickstart_{EMBEDDER.replace('/', '___')}\"\n",
    "checkpoint_dir = os.path.join(OUT_DIR, \"checkpoints\", CACHE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**\n",
    "\n",
    "The dataset we're using here is a subset of 20K Yelp reviews, with 2K reviews used for validation (during SAE training). \n",
    "\n",
    "The target variable is the `stars` column, which is a rating between 1 and 5. We treat this as a regression task.\n",
    "\n",
    "There are also 2K reviews used for holdout evaluation, which we'll use at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "df_train, texts_train = neuro.decoding.get_shared_data_for_subjects(\n",
    "    ['UTS01', 'UTS02', 'UTS03'],\n",
    "    train_or_test='train',\n",
    "    concatenate_running_texts_frac=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.join(os.path.join(config.REPO_DIR, 'data', \"demo_data\"))\n",
    "train_df = pd.read_json(os.path.join(base_dir, \"yelp-demo-train-20K.json\"), lines=True)\n",
    "val_df = pd.read_json(os.path.join(base_dir, \"yelp-demo-val-2K.json\"), lines=True)\n",
    "\n",
    "texts = train_df['text'].tolist()\n",
    "labels = train_df['stars'].values\n",
    "val_texts = val_df['text'].tolist() # These are only used for early stopping of SAE training, so we don't need labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute text embeddings for your dataset**\n",
    "\n",
    "We'll compute text embeddings for a training set, and optionally a validation set. The validation embeddings are used for SAE eval and early-stopping during training.\n",
    "\n",
    "Embeddings will be stored in the `emb_cache` directory (or `os.environ[\"EMB_CACHE_DIR\"]` if you set it) using the `cache_name` parameter, so you only need to compute embeddings once.\n",
    "\n",
    "You can use OpenAI or a local model.\n",
    "\n",
    "Local models will run much faster on GPU. The default local model is `nomic-ai/modernbert-embed-base`. You can use any sentence-transformers model, but please read the model's docs; you may need to edit `get_local_embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text2embedding = get_local_embeddings(texts + val_texts, model=EMBEDDER, batch_size=128, cache_name=CACHE_NAME)\n",
    "# embeddings = np.stack([text2embedding[text] for text in texts])\n",
    "\n",
    "# train_embeddings = np.stack([text2embedding[text] for text in texts])\n",
    "# val_embeddings = np.stack([text2embedding[text] for text in val_texts])\n",
    "# joblib.dump((train_embeddings, val_embeddings), os.path.join(OUT_DIR, f\"{CACHE_NAME}_embeddings.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings, val_embeddings = joblib.load(os.path.join(OUT_DIR, f\"{CACHE_NAME}_embeddings.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train SAE** \n",
    "\n",
    "We will train a Matryoshka SAE with $M=256$, $k=8$, and $\\text{prefix\\_lengths} = [32, 256]$.  \n",
    "\n",
    "With the Matryoshka loss, the SAE will learn to reconstruct the input from (1) just the first 32 neurons, and (2) all 256 neurons.  \n",
    "This will produce 32 coarse-grained features, and 224 finer-grained features.  \n",
    "\n",
    "See the README for more details about selecting SAE hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sae = train_sae(embeddings=train_embeddings, val_embeddings=val_embeddings,\n",
    "#                 M=512, K=16, matryoshka_prefix_lengths=[64, 512], \n",
    "#                 checkpoint_dir=checkpoint_dir)\n",
    "# joblib.dump(sae, os.path.join(checkpoint_dir, \"sae.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = joblib.load(os.path.join(checkpoint_dir, \"sae.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret neurons**  \n",
    "\n",
    "Interpret a random subset of neurons in the SAE to sanity-check that the learned features, and their interpretations, seem reasonable. We generate and print labels for `n_random_neurons` neurons, and we also print out the top-activating texts for each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This instruction will be included in the neuron interpretation prompt.\n",
    "# The below instructions are specific to Yelp, but you can customize this for your task.\n",
    "# If you don't pass in task-specific instructions, there is a generic instruction (see src/interpret_neurons.py);\n",
    "# task-specific instructions are optional, but they help produce hypotheses at the desired level of specificity.\n",
    "\n",
    "# Interpret random neurons\n",
    "results_examples = interpret_sae(\n",
    "    texts=texts,\n",
    "    embeddings=train_embeddings,\n",
    "    sae=sae,\n",
    "    n_random_neurons=5,\n",
    "    print_examples_n=3,\n",
    "    task_specific_instructions=TASK_SPECIFIC_INSTRUCTIONS,\n",
    "    interpreter_model=INTERPRETER_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate hypotheses**\n",
    "\n",
    "Generate hypotheses which are predictive of the target variable.\n",
    "\n",
    "The `selection_method` parameter defines how we compute neuron predictiveness (see `src/select_neurons.py` for more details):\n",
    "- \"separation_score\": E[target | top-activating examples] - E[target | zero-activating examples]\n",
    "- \"correlation\": pearson(neuron activations, target variable)\n",
    "- \"lasso\": select N nonzero features with an L1 regularized model\n",
    "\n",
    "This cell outputs a dataframe with the following columns:\n",
    "- `neuron_idx`: The index of the neuron in the SAE (if you're using multiple SAEs, this will be a global index across all of them).\n",
    "- `source_sae`: The SAE that the neuron was selected from.\n",
    "- `target_{selection_method}`: The predictiveness of the neuron for the target variable, using the selected `selection_method`.\n",
    "- `interpretation`: The natural language interpretation of the neuron.\n",
    "- `interp_fidelity_score`: The F1 fidelity score for how well the neuron's interpretation actually corresponds to its activation pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_multitarget = np.vstack([labels, labels, labels]).T  # Duplicate labels for multi-target selection\n",
    "\n",
    "results = generate_hypotheses(\n",
    "    texts=texts,\n",
    "    labels=labels_multitarget,\n",
    "    embeddings=train_embeddings,\n",
    "    sae=sae,\n",
    "    cache_name=CACHE_NAME,\n",
    "    classification=False,\n",
    "\n",
    "    # hyperparams\n",
    "    selection_method = \"correlation_multi_target\",\n",
    "    n_selected_neurons=20,\n",
    "    n_candidate_interpretations=1,\n",
    "    task_specific_instructions=TASK_SPECIFIC_INSTRUCTIONS,\n",
    "    interpreter_model=INTERPRETER_MODEL,\n",
    "\n",
    "    # provide some scoring of the hypotheses using this many examples\n",
    "    n_scoring_examples=0,\n",
    "    # n_scoring_examples=100,\n",
    "    # annotator_model=ANNOTATOR_MODEL,\n",
    "    # n_workers_annotation=N_WORKERS_ANNOTATION, # Please lower this parameter if you are running into OpenAI API rate limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMost predictive features of Yelp reviews:\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(results.sort_values(by=f\"target_correlation_multi_target\", ascending=False))\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['interpretation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = results['interpretation'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = ['The input mentions long wait times to receive service?', 'Does the input mention rude staff behavior?'] + input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = imodelsx.llm.get_llm('gpt-4.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = neuro.agent.revise_invalid_questions_by_rewording(input_list, lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(qs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imodelsx.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_REWORD = f\"\"\"\n",
    "Rewrite every element in this list to be a question starting with \"Does the input\" and ending with a question mark.\n",
    "\n",
    "- {'\\n- '.join(input_list)}\n",
    "\n",
    "Make as few changes as possible to the original text.\n",
    "Return only a python list and nothing else.\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_list = agent(PROMPT_REWORD, max_completion_tokens=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PROMPT_REWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PROMPT_REWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
