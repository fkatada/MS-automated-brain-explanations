{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import joblib\n",
    "from pprint import pprint\n",
    "from sklearn.preprocessing import normalize\n",
    "from math import ceil\n",
    "import cortex\n",
    "from neuro import config\n",
    "from collections import defaultdict\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from neuro import flatmaps_helper\n",
    "from neuro.flatmaps_helper import load_flatmaps\n",
    "import neuro.sasc.viz\n",
    "import neuro.viz\n",
    "from neuro import analyze_helper\n",
    "import nibabel as nib\n",
    "neurosynth_compare = __import__('04_neurosynth_compare')\n",
    "import neurosynth\n",
    "from neuro.features.questions.gpt4 import QS_35_STABLE\n",
    "from neuro.features import qa_questions\n",
    "import dvu\n",
    "import viz\n",
    "dvu.set_style()\n",
    "\n",
    "config.setup_freesurfer()\n",
    "N_SURVEY_RESPONSES = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensemble (non-gpt-4) feats each run one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_all = pd.read_pickle(join(config.RESULTS_DIR_LOCAL, 'results_full_oct17.pkl'))\n",
    "r = rr_all[rr_all.ndelays == 4]\n",
    "r = r[r.pc_components == 100]\n",
    "r = r[r.feature_space == 'qa_embedder']\n",
    "r = r[r.qa_questions_version == 'v3_boostexamples_merged']\n",
    "r = r[r.qa_embedding_model == 'ensemble2']\n",
    "r = r[r.single_question_idx >= 0]\n",
    "r = r[r.feature_selection_alpha == -1]\n",
    "\n",
    "ravg = r.groupby(['single_question_idx'])[\n",
    "    ['corrs_test_mean']].mean().reset_index()\n",
    "qs = qa_questions.get_merged_questions_v3_boostexamples()\n",
    "ravg['question'] = ravg['single_question_idx'].apply(lambda i: qs[i])\n",
    "ravg['q_selected'] = ravg['question'].apply(lambda q: q in QS_35_STABLE)\n",
    "ravg['question_abbrev'] = ravg['question'].apply(\n",
    "    analyze_helper.abbrev_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize top and bottom questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ravg.to_pickle('corrs_df/single_question_corrs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_selected = ravg[ravg.q_selected]\n",
    "r_unselected = ravg[~ravg.q_selected]\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.histplot(r_unselected.corrs_test_mean, label='Unselected', color=\"C1\")\n",
    "sns.histplot(r_selected.corrs_test_mean, label='Selected 35', color=\"C0\")\n",
    "plt.legend()\n",
    "plt.xlabel('Test correlation using single-question model')\n",
    "plt.ylabel('Question count')\n",
    "print('means', r_selected.corrs_test_mean.mean(),\n",
    "      r_unselected.corrs_test_mean.mean())\n",
    "neuro.viz.savefig(\n",
    "    'monosemantic/single_question_perf_hists.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_colwidth', None,\n",
    "#                        'display.max_rows', None):\n",
    "#     display(ravg[~ravg.q_selected].sort_values('corrs_test_mean', ascending=False)\n",
    "#             [['question_abbrev', 'corrs_test_mean']].head(20))\n",
    "print(\n",
    "    ravg[~ravg.q_selected]\n",
    "    .sort_values('corrs_test_mean', ascending=False)[['question_abbrev', 'corrs_test_mean']]\n",
    "    .head(15).to_latex(float_format=\"%.3f\", index=False)\n",
    ")\n",
    "print(\n",
    "    ravg[~ravg.q_selected]\n",
    "    .sort_values('corrs_test_mean', ascending=False)[['question_abbrev', 'corrs_test_mean']]\n",
    "    .tail(15).to_latex(float_format=\"%.3f\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load survey results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_results = pd.read_csv('survey_results.csv')\n",
    "# set first column name to 'question_abbrev'\n",
    "survey_results.rename(columns={survey_results.columns[0]: 'question_abbrev'}, inplace=True)\n",
    "\n",
    "# apply lambda function to col names\n",
    "survey_results.columns = [x[:x.index('(')].strip() if '(' in x else x for x in survey_results.columns]\n",
    "# add question mark to question_abbrev\n",
    "\n",
    "def remove_parens(s):\n",
    "    if '(' in s and ')' in s:\n",
    "        # remove everything from the first '(' to the first ')'\n",
    "        # and return the rest of the string\n",
    "        s = s[:s.index('(')] + s[s.index(')') + 1:]\n",
    "    return s\n",
    "\n",
    "survey_results['question_abbrev'] = survey_results['question_abbrev'].apply(\n",
    "    remove_parens)\n",
    "survey_results['question_abbrev'] = survey_results['question_abbrev'].apply(\n",
    "    lambda x: x.strip() + '?' if not x.strip().endswith('?') else x)\n",
    "\n",
    "RENAME_DICT = {\n",
    "    '...contain numbers?': '...contain a number?',\n",
    "    \"...are reflective, involving self-analysis or introspection?\":\"...reflective, involving self-analysis or introspection?\",\n",
    "    \"...are related to a specific industry or profession?\":\n",
    "        \"...related to a specific industry or profession?\",\n",
    "     \"...is abstract rather than concrete?\":\n",
    "        \"...abstract rather than concrete?\",\n",
    "    \"...describe an interpersonal misunderstanding or dispute?\":\n",
    "        \"...describe a an interpersonal misunderstanding or dispute?\",\n",
    "     \"...cointain first-person pronoun?\":\n",
    "        \"...first-person pronoun in the input?\",\n",
    "    \"...are part of a legal document or text?\":\n",
    "        \"...part of a legal document or text?\",\n",
    "    '...include a description about dialogue?':    \n",
    "        '...include dialogue?',\n",
    "    '...describe an educational lesson or class?':\n",
    "        '...educational lesson or class described?',\n",
    "}\n",
    "# rename columns according to RENAME_DICT\n",
    "survey_results['question_abbrev'] = [RENAME_DICT.get(x, x) for x in survey_results['question_abbrev']]\n",
    "\n",
    "\n",
    "# merge with ravg on question_abbrev\n",
    "merged = ravg.merge(survey_results, on='question_abbrev', how='right')\n",
    "\n",
    "# for any duplicate question_abbrev, take the one where q_selected is True\n",
    "merged = merged.sort_values('q_selected', ascending=False).drop_duplicates(\n",
    "    'question_abbrev').sort_index()\n",
    "\n",
    "merged['Standard Error'] = merged['Standard Deviation'] / np.sqrt(N_SURVEY_RESPONSES)\n",
    "merged['legend'] = merged['q_selected'].apply(\n",
    "    lambda x: 'Selected 35' if x else 'Unselected')\n",
    "merged.sort_values('corrs_test_mean', ascending=False, inplace=True)\n",
    "merged['category'] = merged['question'].apply(lambda x: viz.REMAP_QUESTIONS_TO_CATEGORY_NAMES.get(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.416)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[merged['corrs_test_mean'] < 0.01]['Average'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.374, p-value: 0.017\n"
     ]
    }
   ],
   "source": [
    "vals = merged[merged['corrs_test_mean'] > 0.01][['Average', 'corrs_test_mean']]\n",
    "\n",
    "# compute correlation and p-value\n",
    "from scipy.stats import pearsonr\n",
    "corr, pval = pearsonr(vals['Average'], vals['corrs_test_mean'])\n",
    "print(f'Correlation: {corr:.3f}, p-value: {pval:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merged = merged[['question_abbrev', 'Average', 'Standard Deviation', 'corrs_test_mean']].sort_values(by='corrs_test_mean', ascending=False)\n",
    "# with pd.option_context('display.max_colwidth', None,\n",
    "#                        'display.max_rows', None):\n",
    "#     display(merged[merged.corrs_test_mean.isna()]['question_abbrev'])\n",
    "#     # display(ravg.question_abbrev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.errorbar(merged['corrs_test_mean'], merged['Average'],\n",
    "             yerr=merged['Standard Error'], fmt='none', capsize=2,\n",
    "             elinewidth=1, alpha=0.35, color='gray', zorder=-11)\n",
    "sns.scatterplot(data=merged, x='corrs_test_mean', y='Average',\n",
    "                hue='category',\n",
    "                style='legend',\n",
    "                hue_order=[\n",
    "                    'Visuospatial', 'Communication',\n",
    "                'Beliefs, values, emotions', 'Numeric',\n",
    "                'Tactile', 'Other', ],\n",
    "                style_order=['Selected 35', 'Unselected'],\n",
    "                s=100, alpha=0.9)\n",
    "plt.xlabel('Test correlation using single-question model')\n",
    "plt.ylabel('Expert rating')\n",
    "plt.legend(title='', frameon=False, handletextpad=0.1)\n",
    "# put legend outside\n",
    "plt.legend(title='', frameon=False, handletextpad=0.1, loc='upper left',\n",
    "           bbox_to_anchor=(1, 1), ncol=1)\n",
    "neuro.viz.savefig(\n",
    "    'monosemantic/single_question_perf_vs_survey.png', bbox_inches='tight', dpi=400)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
