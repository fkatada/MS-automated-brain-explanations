{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HypotheSAEs Quickstart\n",
    "\n",
    "This notebook demonstrates basic usage of HypotheSAEs on a sample of the Yelp review dataset.  \n",
    "We use GPT-4.1 for hypothesis generation (interpreting neurons), and GPT-4.1-mini for text annotation.  \n",
    "Please set your OpenAI API key in the environment variable `OPENAI_KEY_SAE` in the below notebook cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "# os.environ['OPENAI_KEY_SAE'] = '...' # Replace with your OpenAI API key, or with another environment variable (e.g. os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from neuro.baselines.hypothesaes.quickstart import train_sae, interpret_sae, generate_hypotheses, evaluate_hypotheses\n",
    "from neuro.baselines.hypothesaes.embedding import get_openai_embeddings, get_local_embeddings\n",
    "\n",
    "OUT_DIR = os.path.expanduser('~/mntv1/deep-fMRI/qa/hypothesaes')\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "INTERPRETER_MODEL = \"gpt-4.1\"\n",
    "ANNOTATOR_MODEL = \"gpt-4.1-mini\"\n",
    "# EMBEDDER = \"text-embedding-3-small\" # OpenAI\n",
    "EMBEDDER = \"nomic-ai/modernbert-embed-base\" # Huggingface model, will run locally\n",
    "CACHE_NAME = f\"yelp_quickstart_{EMBEDDER}\"\n",
    "os.environ['EMB_CACHE_DIR'] = os.path.join(OUT_DIR, os.path.expanduser('~/emb_cache'))\n",
    "N_WORKERS_ANNOTATION = 30 # Number of parallel threads to use for annotation API calls; lower if hitting OpenAI rate limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**\n",
    "\n",
    "The dataset we're using here is a subset of 20K Yelp reviews, with 2K reviews used for validation (during SAE training). \n",
    "\n",
    "The target variable is the `stars` column, which is a rating between 1 and 5. We treat this as a regression task.\n",
    "\n",
    "There are also 2K reviews used for holdout evaluation, which we'll use at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '../data'\n",
    "\n",
    "base_dir = os.path.join(prefix, \"demo_data\")\n",
    "train_df = pd.read_json(os.path.join(base_dir, \"yelp-demo-train-20K.json\"), lines=True)\n",
    "val_df = pd.read_json(os.path.join(base_dir, \"yelp-demo-val-2K.json\"), lines=True)\n",
    "\n",
    "texts = train_df['text'].tolist()\n",
    "labels = train_df['stars'].values\n",
    "val_texts = val_df['text'].tolist() # These are only used for early stopping of SAE training, so we don't need labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute text embeddings for your dataset**\n",
    "\n",
    "We'll compute text embeddings for a training set, and optionally a validation set. The validation embeddings are used for SAE eval and early-stopping during training.\n",
    "\n",
    "Embeddings will be stored in the `emb_cache` directory (or `os.environ[\"EMB_CACHE_DIR\"]` if you set it) using the `cache_name` parameter, so you only need to compute embeddings once.\n",
    "\n",
    "You can use OpenAI or a local model.\n",
    "\n",
    "Local models will run much faster on GPU. The default local model is `nomic-ai/modernbert-embed-base`. You can use any sentence-transformers model, but please read the model's docs; you may need to edit `get_local_embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text2embedding = get_openai_embeddings(texts + val_texts, model=EMBEDDER, cache_name=CACHE_NAME)\n",
    "text2embedding = get_local_embeddings(texts + val_texts, model=EMBEDDER, batch_size=128, cache_name=CACHE_NAME)\n",
    "embeddings = np.stack([text2embedding[text] for text in texts])\n",
    "\n",
    "train_embeddings = np.stack([text2embedding[text] for text in texts])\n",
    "val_embeddings = np.stack([text2embedding[text] for text in val_texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train SAE** \n",
    "\n",
    "We will train a Matryoshka SAE with $M=256$, $k=8$, and $\\text{prefix\\_lengths} = [32, 256]$.  \n",
    "\n",
    "With the Matryoshka loss, the SAE will learn to reconstruct the input from (1) just the first 32 neurons, and (2) all 256 neurons.  \n",
    "This will produce 32 coarse-grained features, and 224 finer-grained features.  \n",
    "\n",
    "See the README for more details about selecting SAE hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ddd8ad9bc7462191beadcdcf1a1b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 98 epochs\n",
      "Saved model to /home/chansingh/mntv1/deep-fMRI/qa/hypothesaes/checkpoints/yelp_quickstart_nomic-ai/modernbert-embed-base/SAE_matryoshka_M=512_K=16_prefixes=64-512.pt\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join(OUT_DIR, \"checkpoints\", CACHE_NAME)\n",
    "sae = train_sae(embeddings=train_embeddings, val_embeddings=val_embeddings,\n",
    "                M=512, K=16, matryoshka_prefix_lengths=[64, 512], \n",
    "                checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret neurons**  \n",
    "\n",
    "Interpret a random subset of neurons in the SAE to sanity-check that the learned features, and their interpretations, seem reasonable. We generate and print labels for `n_random_neurons` neurons, and we also print out the top-activating texts for each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702ba59bdeb442f1a7329da13bb4b4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing activations (batchsize=16384):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations shape: (20000, 512)\n",
      "Failed to get interpretation: Please set the OPENAI_KEY_SAE environment variable before using functions which require the OpenAI API.\n",
      "Failed to get interpretation: Please set the OPENAI_KEY_SAE environment variable before using functions which require the OpenAI API.\n",
      "Failed to get interpretation: Please set the OPENAI_KEY_SAE environment variable before using functions which require the OpenAI API.\n",
      "Failed to get interpretation: Please set the OPENAI_KEY_SAE environment variable before using functions which require the OpenAI API.\n",
      "Failed to get interpretation: Please set the OPENAI_KEY_SAE environment variable before using functions which require the OpenAI API.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964d2d8091ae4e5dabe803e4180576a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating interpretations:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neuron 428 (0.0% active, from SAE M=512, K=16): None\n",
      "\n",
      "Top activating examples:\n",
      "1. Stopped in for a bite and I am so disappointed. Sat at the bar and everything seemed fine. Ordered our food and noticed the bartender was completely messed up out of his mind. He literally asked us 5 times if we were ready to order after he put our order in. He was so incredibly high. My order came out and it was all wrong. When I asked for the bill he had the nerve to ask what I ordered. Told him and he still got the check wrong. Never coming back due to the face I'm scared the bartender might have an overdose in front of me. A manager was there but I don't think he cared or he was just as high.\n",
      "2. Fantastic and personable bartenders and great food that doesn't leave you feeling gross or heavy\n",
      "3. If not for the second bartender this would have been a one star review. First bartender was inattentive and didn't listen when I asked for water initially, so that took about 10 minutes to get when she came by a second time. Thankfully a second bartender started her shift and I was finally able to put in my food order. Ordered the pizzadilla and it was not as advertised. It came out room temperature almost like it was microwaved. The toppings on the pizzadilla were falling off like a pizza not ran through an oven would do. Food and service were subpar. I probably will never go here again.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Neuron 133 (0.0% active, from SAE M=512, K=16): None\n",
      "\n",
      "Top activating examples:\n",
      "1. This location is horrible! Especially at night. My boyfriend and I decided to go thru the drive thru. We waited at the screen for a couple minutes before we could even order. When we did everything looked correct on the screen so we drove ahead. We waited in line for our food at least ten minutes! We get our food check the bag and all looks good so drive home. Get home half of our order is incorrect and the other half soggy! Drive back up there and get a refund bc at that point after waiting and dealing with poor customer service decided to go to the lemay ferry location. It was a great decision. Food at the second location was fresh, accurate, fast and hot. Long story short - don't waste your time at this location and go to the lemay ferry location.\n",
      "2. I went to this location TWICE and both times the service was terrible. Fool me once, shame on you..TWICE?? Never again......save yourself the disappointment and go to another location. There's one not far from there on Veterans right next door to Barnes and Noble. Grab a bite to eat and a good book to read.\n",
      "3. I have visited this location on a few different occasions, and this review reflects an average of my experiences. Also, 3 stars equals A-OK on the yelp scale. In the past, this location used to have issues with not being open during posted hours and lower quality than the norm for subway. It has been a few months since I visited this location, but a year ago I among other customers was somewhat dismayed by this.  My most recent visit was during the middle of the day as opposed to morning or later evening, which in the past was when I had found the location closed when they should be open.  With that said, during my most recent visit, I did notice a major change for the better. Better attitudes, fresher foods on par with subway standards, prompt service. I liked my food and will come back in hopes that this location has made permanent improvements.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Neuron 424 (0.0% active, from SAE M=512, K=16): None\n",
      "\n",
      "Top activating examples:\n",
      "1. Moved from Seattle to St. Louis with a definite apprehension about the food scene here relative to the PNW. This place definitely gives me hope that St. Louis can hold it's own.   Had one of the best Bi bim baps I've eaten in some time plus a smoothie that was lights out. Space is also pretty cool. Keep up the great work guys!\n",
      "2. I've been to St. Louis twice and here twice.  Love the food.  The Slinger is ridiculously good.\n",
      "3. Great food, great drinks, open late! We tried this place after having a sub-par experience somewhere else, and we were pleasantly surprised! Close proximity to our hotel was a definite plus!  We will most definitely be back the next time we're in St. Louis!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Neuron 497 (0.0% active, from SAE M=512, K=16): None\n",
      "\n",
      "Top activating examples:\n",
      "1. Went to Zama last night with a few girlfriends and had a really nice meal.  Parking wasn't too bad but I bet on weekends it's awful.  We had a bunch of little appetizers, sushi and the dessert sampler.  Everything was really tasty and the service and atmosphere were excellent.  I will definitely be back for another fab dinner.  It's nice to find somewhere to eat that's reasonably priced  in the city that isn't a pizza shop.\n",
      "2. I love this Zoe's Kitchen.  It's the cleanest I've ever been to, and the staff is very friendly.  I've always gotten my food quickly.  This is my go to restaurant, love it!!\n",
      "3. Zesto's is consistently great and fast.  We tend to order their pizza and their wings which are both great but we've ordered cheese steaks, gyros and fries and they're consistent in their quality.  If you're looking to be impressed this is the way to go.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Neuron 205 (0.0% active, from SAE M=512, K=16): None\n",
      "\n",
      "Top activating examples:\n",
      "1. Terribly slow drive through service. I know they're new but they really need to step up their game!\n",
      "2. 8:40am on Monday MAY 3rd. The woman wearing a leopard shirt working the drive-through absolutely terrible service. Just an attitude behind everything she says. I got to the drive-through and waited two minutes with my window down while it's raining. She made me repeat everything twice having to literally shout. Unacceptable. I get to the window to pay him I wait another minute and a half waiting for them to turn around and finally acknowledged I rolled up. And what I get  greeted with is more attitude. I'm giving three stars because the food is usually good and the staff usually isn't a pain in the ass like this woman. Fix your damn drive-through machine and hire people with the personality greater than a fly!\n",
      "3. This is literally the slowest drive thru I've ever experienced. They average about 1 car every 10-15 minutes. The food is decent but definitely not worth such an extreme wait. They need to figure out a solution to the wait time pronto!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# This instruction will be included in the neuron interpretation prompt.\n",
    "# The below instructions are specific to Yelp, but you can customize this for your task.\n",
    "# If you don't pass in task-specific instructions, there is a generic instruction (see src/interpret_neurons.py);\n",
    "# task-specific instructions are optional, but they help produce hypotheses at the desired level of specificity.\n",
    "\n",
    "TASK_SPECIFIC_INSTRUCTIONS = \"\"\"All of the texts are reviews of restaurants on Yelp.\n",
    "Features should describe a specific aspect of the review. For example:\n",
    "- \"mentions long wait times to receive service\"\n",
    "- \"praises how a dish was cooked, with phrases like 'perfect medium-rare'\\\"\"\"\"\n",
    "\n",
    "# Interpret random neurons\n",
    "results = interpret_sae(\n",
    "    texts=texts,\n",
    "    embeddings=train_embeddings,\n",
    "    sae=sae,\n",
    "    n_random_neurons=5,\n",
    "    print_examples_n=3,\n",
    "    task_specific_instructions=TASK_SPECIFIC_INSTRUCTIONS,\n",
    "    interpreter_model=INTERPRETER_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate hypotheses**\n",
    "\n",
    "Generate hypotheses which are predictive of the target variable.\n",
    "\n",
    "The `selection_method` parameter defines how we compute neuron predictiveness (see `src/select_neurons.py` for more details):\n",
    "- \"separation_score\": E[target | top-activating examples] - E[target | zero-activating examples]\n",
    "- \"correlation\": pearson(neuron activations, target variable)\n",
    "- \"lasso\": select N nonzero features with an L1 regularized model\n",
    "\n",
    "This cell outputs a dataframe with the following columns:\n",
    "- `neuron_idx`: The index of the neuron in the SAE (if you're using multiple SAEs, this will be a global index across all of them).\n",
    "- `source_sae`: The SAE that the neuron was selected from.\n",
    "- `target_{selection_method}`: The predictiveness of the neuron for the target variable, using the selected `selection_method`.\n",
    "- `interpretation`: The natural language interpretation of the neuron.\n",
    "- `interp_fidelity_score`: The F1 fidelity score for how well the neuron's interpretation actually corresponds to its activation pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_method = \"correlation\"\n",
    "results = generate_hypotheses(\n",
    "    texts=texts,\n",
    "    labels=labels,\n",
    "    embeddings=embeddings,\n",
    "    sae=sae,\n",
    "    cache_name=CACHE_NAME,\n",
    "    selection_method=selection_method,\n",
    "    n_selected_neurons=20,\n",
    "    n_candidate_interpretations=1,\n",
    "    task_specific_instructions=TASK_SPECIFIC_INSTRUCTIONS,\n",
    "    interpreter_model=INTERPRETER_MODEL,\n",
    "    annotator_model=ANNOTATOR_MODEL,\n",
    "    n_workers_annotation=N_WORKERS_ANNOTATION, # Please lower this parameter if you are running into OpenAI API rate limits\n",
    ")\n",
    "\n",
    "print(\"\\nMost predictive features of Yelp reviews:\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(results.sort_values(by=f\"target_{selection_method}\", ascending=False))\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate held-out generalization**\n",
    "\n",
    "Finally, we evaluate whether these are good hypotheses by testing whether their natural language interpretations can predict the target variable.  \n",
    "\n",
    "We compute annotations for each hypothesized concept on a holdout set (not seen during SAE training & feature selection).\n",
    "\n",
    "After annotation, we output a dataframe with the following columns:\n",
    "- `hypothesis`: The natural language hypothesis (which came from interpreting a predictive neuron in the SAE)\n",
    "- `separation_score`: How much the target variable differs when the concept is present vs. absent (i.e., $E[Y\\mid\\text{concept} = 1] - E[Y\\mid\\text{concept} = 0]$).\n",
    "- `separation_pvalue`: The t-test p-value of the null hypothesis that the separation score is 0 (i.e., the concept is not associated with the target variable).\n",
    "- `regression_coef`: The coefficient of the concept in a multivariate linear regression of the target variable on all concepts.\n",
    "- `regression_pval`: The p-value of the null hypothesis that the regression coefficient is 0.\n",
    "- `feature_prevalence`: The fraction of examples that contain the concept.\n",
    "\n",
    "Additionally, we output the evaluation metrics used in the paper:\n",
    "- Significant hypotheses: the number of hypotheses that are significant in the multivariate regression at a specified significance level (default $0.1$) after Bonferroni correction. You can pass in a different significance level using the `corrected_pval_threshold` parameter.\n",
    "- AUC or $R^2$: how well the hypotheses collectively predict the target variable in the multivariate regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_df = pd.read_json(os.path.join(base_dir, \"yelp-demo-holdout-2K.json\"), lines=True)\n",
    "holdout_texts = holdout_df['text'].tolist()\n",
    "holdout_labels = holdout_df['stars'].values\n",
    "\n",
    "metrics, evaluation_df = evaluate_hypotheses(\n",
    "    hypotheses_df=results,\n",
    "    texts=holdout_texts,\n",
    "    labels=holdout_labels,\n",
    "    cache_name=CACHE_NAME,\n",
    "    annotator_model=ANNOTATOR_MODEL,\n",
    "    n_workers_annotation=N_WORKERS_ANNOTATION, # Please lower this parameter if you are running into OpenAI API rate limits\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(evaluation_df)\n",
    "pd.reset_option('display.max_colwidth')\n",
    "\n",
    "print(\"\\nHoldout Set Metrics:\")\n",
    "print(f\"R² Score: {metrics['r2']:.3f}\")\n",
    "print(f\"Significant hypotheses: {metrics['Significant'][0]}/{metrics['Significant'][1]} \" \n",
    "      f\"(p < {metrics['Significant'][2]:.3e})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
