{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "# import sasc.notebook_helper\n",
    "import sasc.viz\n",
    "import scipy.special\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import sasc.analyze_helper\n",
    "import joblib\n",
    "# import viz\n",
    "from sasc.config import RESULTS_DIR\n",
    "\n",
    "setting = 'default'\n",
    "# setting = 'roi'\n",
    "MAIN_DIR = join(RESULTS_DIR, 'stories', setting)\n",
    "EXPT_DIRS = sorted([join(MAIN_DIR, dir_name) for dir_name in os.listdir(MAIN_DIR)])\n",
    "# EXPT_DIRS = [x for x in EXPT_DIRS if 'uts03' in x.lower() or 'uts01' in x.lower()]\n",
    "# EXPT_DIRS = [x for x in EXPT_DIRS if 'uts02' in x.lower()]\n",
    "EXPT_DIRS = [x for x in EXPT_DIRS if 'uts01' in x.lower()]\n",
    "EXPT_DIR = EXPT_DIRS[0]\n",
    "\n",
    "# things are precomputed using 02_process_story, so we don't need this now\n",
    "# rows = joblib.load(join(EXPT_DIR, f'rows.pkl'))\n",
    "# expls = rows.expl.values\n",
    "# try:\n",
    "    # prompts_paragraphs = joblib.load(\n",
    "        # join(EXPT_DIR, \"prompts_paragraphs.pkl\"),\n",
    "    # )\n",
    "    # prompts = prompts_paragraphs[\"prompts\"]\n",
    "    # paragraphs = prompts_paragraphs[\"paragraphs\"]\n",
    "# except:\n",
    "    # prompts = open(join(EXPT_DIR, \"prompts.txt\"), 'r').read().split('\\n\\n')\n",
    "    # paragraphs = open(join(EXPT_DIR, \"story.txt\"), 'r').read().split('\\n\\n')\n",
    "    # assert len(prompts) == len(paragraphs), f\"{len(prompts)} != {len(paragraphs)}\"\n",
    "# voxel_nums = rows.module_num.values\n",
    "# subjects = rows.subject.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data** <> Story Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_final = False\n",
    "annotate_labels = False\n",
    "for EXPT_DIR in EXPT_DIRS:\n",
    "    try:\n",
    "        d = joblib.load(join(EXPT_DIR, \"scores_data.pkl\"))\n",
    "    except:\n",
    "        print('skipping', EXPT_DIR)\n",
    "        continue\n",
    "    s = d[\"scores_mean\"].T\n",
    "    # normalize each column\n",
    "    s = (s - s.mean(axis=0)) / \\\n",
    "        s.std(axis=0)\n",
    "    s = s.T\n",
    "\n",
    "    try:\n",
    "        rows = joblib.load(join(EXPT_DIR, f\"rows.pkl\"))\n",
    "    except:\n",
    "        rows = pd.read_csv(join(EXPT_DIR, f\"rows.csv\"))\n",
    "    # rows = joblib.load(join(EXPT_DIR, f\"rows.pkl\"))\n",
    "    expls = rows.expl.values\n",
    "    if order_final:\n",
    "        expls_order = np.argsort(expls)\n",
    "        expls_order = expls_order[[15, 7, 11, 14, 3,\n",
    "                                   12, 4, 1, 2, 0, 13, 6, 5, 16, 10, 9, 8]]\n",
    "    else:\n",
    "        if setting == 'default':\n",
    "            expls_order = sasc.analyze_helper.sort_expls_semantically(expls)\n",
    "            s = s[expls_order][:, expls_order]  # [:, expls_order]\n",
    "            expls = expls[expls_order]\n",
    "    # {\"scores_mean\": scores_mean, \"scores_all\": scores_all},\n",
    "    sasc.viz.heatmap(s, expls, ylab=\"Voxel explanation\", diverging=True,\n",
    "                     xlab=\"Driving paragraph\", clab='Ngram matching score', label_fontsize='large')\n",
    "    sasc.viz.outline_diagonal(s.shape, color='black', lw=1, block_size=1)\n",
    "    labs = [f'{i + 1:02d}' for i in range(len(expls))]\n",
    "    fontsize = 'medium'\n",
    "    if annotate_labels:\n",
    "        plt.yticks(labels=expls, ticks=np.arange(\n",
    "            len(expls)), fontsize=fontsize)\n",
    "        plt.xticks(labels=expls, ticks=np.arange(\n",
    "            len(expls)), rotation=90, fontsize=fontsize)\n",
    "    else:\n",
    "        plt.yticks(labels=labs, ticks=np.arange(\n",
    "            len(labs)), fontsize=fontsize)\n",
    "        plt.xticks(labels=labs, ticks=np.arange(\n",
    "            len(labs)), rotation=90, fontsize=fontsize)\n",
    "\n",
    "    plt.savefig(join(EXPT_DIR, f\"story_data_match.pdf\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module** <> Story Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for EXPT_DIR in EXPT_DIRS:\n",
    "    scores_data = joblib.load(\n",
    "        join(EXPT_DIR, f\"scores_mod_ngram_length={0}.pkl\"))\n",
    "    s = scores_data[\"scores_mean\"].T\n",
    "    # z-score each column\n",
    "    s = (s - s.mean(axis=0)) / s.std(axis=0)\n",
    "    s = s.T\n",
    "\n",
    "    # s = scipy.special.softmax(s, axis=0)\n",
    "    # s = (s - s.min()) / (s.max() - s.min())\n",
    "    # rows = joblib.load(join(EXPT_DIR, f'rows1_rep.pkl'))\n",
    "    try:\n",
    "        rows = joblib.load(join(EXPT_DIR, f\"rows.pkl\"))\n",
    "    except:\n",
    "        rows = pd.read_csv(join(EXPT_DIR, f\"rows.csv\"))\n",
    "    expls = rows.expl.values\n",
    "\n",
    "    if setting == 'default':\n",
    "        expls_order = sasc.analyze_helper.sort_expls_semantically(expls)\n",
    "        s = s[expls_order][:, expls_order]  # [:, expls_order]\n",
    "        expls = expls[expls_order]\n",
    "\n",
    "    sasc.viz.heatmap(\n",
    "        s, expls, ylab=\"Voxel explanation\",\n",
    "        xlab=\"Driving paragraph\", label_fontsize='large', clab='Mean model response ($\\sigma$)')\n",
    "    sasc.viz.outline_diagonal(s.shape, color='black', lw=1, block_size=1)\n",
    "\n",
    "    labs = [f'{i + 1:02d}' for i in range(len(expls))]\n",
    "    fontsize = 'medium'\n",
    "    plt.yticks(labels=labs, ticks=np.arange(\n",
    "        len(labs)), fontsize=fontsize)\n",
    "    plt.xticks(labels=labs, ticks=np.arange(\n",
    "        len(labs)), rotation=90, fontsize=fontsize)\n",
    "    # rois = rows.roi.values\n",
    "    # plt.yticks(labels=[expls[i] + f' ({rois[i]})' for i in range(len(expls))], ticks=np.arange(\n",
    "    #     len(labs)), fontsize=fontsize)\n",
    "    # plt.xticks(labels=expls, ticks=np.arange(\n",
    "    #     len(labs)), rotation=90, fontsize=fontsize)\n",
    "    # ax = plt.gca()\n",
    "    # ax.yaxis.label.set_size('large')\n",
    "    # ax.xaxis.label.set_size('large')\n",
    "    # increase colorbar label size\n",
    "    # cbar = ax.collections[0].colorbar\n",
    "    # cbar.ax.tick_params(labelsize='large')\n",
    "\n",
    "    # calculate mean of diagonal - mean of off-diagonal for s\n",
    "    diag_diff = (\n",
    "        np.mean(np.diag(s))\n",
    "        - (\n",
    "            np.mean(s[np.triu_indices_from(s, k=1)])\n",
    "            + np.mean(s[np.tril_indices_from(s, k=-1)])\n",
    "        )\n",
    "        / 2\n",
    "    ).round(5)\n",
    "    # if not setting == 'default':\n",
    "    plt.title(os.path.basename(EXPT_DIR) + ' diag_diff=' + str(diag_diff))\n",
    "    res[os.path.basename(EXPT_DIR)] = diag_diff\n",
    "\n",
    "    # plt.savefig(join(EXPT_DIR, f\"story_module_match.png\"), dpi=300)\n",
    "    plt.savefig(join(EXPT_DIR, f\"story_module_match.pdf\"), bbox_inches=\"tight\")\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "    # break\n",
    "# sasc.viz.save_figs_to_single_pdf('../results/expl_story_match.pdf')\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaged heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process encoding score averaged across stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = 'default'\n",
    "MAIN_DIR = join(RESULTS_DIR, 'stories', setting)\n",
    "EXPT_DIRS = sorted([join(MAIN_DIR, dir_name)\n",
    "                   for dir_name in os.listdir(MAIN_DIR)])\n",
    "\n",
    "\n",
    "dfs = []\n",
    "mat_dict = {}\n",
    "expls_dict = {}\n",
    "for subject in ['uts01', 'uts03', 'uts02']:  # , 'uts02', 'uts03']:\n",
    "    mats = []\n",
    "    expt_dirs = [x for x in EXPT_DIRS if subject in x]\n",
    "    res = {}\n",
    "\n",
    "    for expt_dir in expt_dirs:\n",
    "\n",
    "        scores_data = joblib.load(\n",
    "            join(expt_dir, f\"scores_mod_ngram_length={0}.pkl\"))\n",
    "        s = scores_data[\"scores_mean\"].T\n",
    "        # z-score each column\n",
    "        s = (s - s.mean(axis=0)) / s.std(axis=0)\n",
    "        s = s.T\n",
    "        try:\n",
    "            rows = joblib.load(join(expt_dir, f\"rows.pkl\"))\n",
    "        except:\n",
    "            rows = pd.read_csv(join(expt_dir, f\"rows.csv\"))\n",
    "        module_nums = rows.module_num.values\n",
    "        expls = rows.expl.values\n",
    "        expls_order = np.argsort(expls)\n",
    "        s = s[expls_order][:, expls_order]  # [:, expls_order]\n",
    "        expls = expls[expls_order]\n",
    "        module_nums = module_nums[expls_order]\n",
    "\n",
    "        # sasc.viz.heatmap(s, expls, ylab=\"Voxel encoding model\",\n",
    "        #  xlab=\"Driving paragraph\", label_fontsize='large', clab='Mean model response ($\\sigma_f$)')\n",
    "        mats.append(s)\n",
    "\n",
    "    mat = np.array(mats).mean(axis=0)\n",
    "    mat_diffs = []\n",
    "    for i in range(mat.shape[0]):\n",
    "        mat_diffs.append(\n",
    "            mat[i][i] - (mat[i].sum() - mat[i][i]) / (mat.shape[0] - 1)\n",
    "        )\n",
    "    df = pd.DataFrame(mat_diffs, columns=['encoding_score'])\n",
    "    df['expl'] = expls\n",
    "    df['module_num'] = module_nums\n",
    "    df['subject'] = subject\n",
    "    dfs.append(df)\n",
    "    mat_dict[subject] = mat\n",
    "    expls_dict[subject] = expls\n",
    "joblib.dump(pd.concat(dfs), join(RESULTS_DIR,\n",
    "            'processed', 'encoding_scores.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expls_order = sasc.analyze_helper.sort_expls_semantically(expls)\n",
    "# expls_order = idx_order\n",
    "for subject in ['uts01', 'uts02', 'uts03']:\n",
    "    expls = expls_dict[subject]\n",
    "    mat = mat_dict[subject]\n",
    "    explanations = True\n",
    "\n",
    "    expls_order = [15, 7, 11, 14, 3, 12, 4, 1, 2, 0, 13, 6, 5, 16, 10, 9, 8]\n",
    "    s = mat[expls_order][:, expls_order]  # [:, expls_order]\n",
    "    sasc.viz.heatmap(\n",
    "        s, expls, ylab=\"Voxel explanation\",\n",
    "        xlab=\"Driving paragraph\", label_fontsize='large', clab='Mean model response ($\\sigma$)')\n",
    "    sasc.viz.outline_diagonal(s.shape, color='black', lw=1, block_size=1)\n",
    "    labs = [f'{i + 1:02d}' for i in range(len(expls))]\n",
    "    fontsize = 'medium'\n",
    "    if explanations:\n",
    "        expls = expls[expls_order]\n",
    "        plt.yticks(labels=expls, ticks=np.arange(\n",
    "            len(labs)), fontsize=fontsize)\n",
    "        plt.xticks(labels=expls, ticks=np.arange(\n",
    "            len(labs)), rotation=90, fontsize=fontsize)\n",
    "\n",
    "    else:\n",
    "        plt.yticks(labels=labs, ticks=np.arange(\n",
    "            len(labs)), fontsize=fontsize)\n",
    "        plt.xticks(labels=labs, ticks=np.arange(\n",
    "            len(labs)), rotation=90, fontsize=fontsize)\n",
    "\n",
    "    plt.savefig(join(RESULTS_DIR, 'figs', 'main',\n",
    "                f'encoding_heatmap_avg_{subject}.pdf'), bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data score averaged across stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = 'default'\n",
    "MAIN_DIR = join(RESULTS_DIR, 'stories', setting)\n",
    "EXPT_DIRS = sorted([join(MAIN_DIR, dir_name)\n",
    "                   for dir_name in os.listdir(MAIN_DIR)])\n",
    "\n",
    "\n",
    "dfs = []\n",
    "mat_dict = {}\n",
    "expls_dict = {}\n",
    "for subject in ['uts01', 'uts03', 'uts02']:  # , 'uts02', 'uts03']:\n",
    "    mats = []\n",
    "    expt_dirs = [x for x in EXPT_DIRS if subject in x]\n",
    "    res = {}\n",
    "\n",
    "    for expt_dir in expt_dirs:\n",
    "\n",
    "        try:\n",
    "            d = joblib.load(join(EXPT_DIR, \"scores_data.pkl\"))\n",
    "        except:\n",
    "            print('skipping', EXPT_DIR)\n",
    "            continue\n",
    "        s = d[\"scores_mean\"].T\n",
    "        # normalize each column\n",
    "        s = (s - s.mean(axis=0)) / \\\n",
    "            s.std(axis=0)\n",
    "        s = s.T\n",
    "\n",
    "        # try:\n",
    "        #     rows = joblib.load(join(EXPT_DIR, f\"rows.pkl\"))\n",
    "        # except:\n",
    "        #     rows = pd.read_csv(join(EXPT_DIR, f\"rows.csv\"))\n",
    "\n",
    "        # scores_data = joblib.load(\n",
    "        #     join(expt_dir, f\"scores_mod_ngram_length={0}.pkl\"))\n",
    "        # s = scores_data[\"scores_mean\"].T\n",
    "        # # z-score each column\n",
    "        # s = (s - s.mean(axis=0)) / s.std(axis=0)\n",
    "        # s = s.T\n",
    "        try:\n",
    "            rows = joblib.load(join(expt_dir, f\"rows.pkl\"))\n",
    "        except:\n",
    "            rows = pd.read_csv(join(expt_dir, f\"rows.csv\"))\n",
    "        module_nums = rows.module_num.values\n",
    "        expls = rows.expl.values\n",
    "        expls_order = np.argsort(expls)\n",
    "        s = s[expls_order][:, expls_order]  # [:, expls_order]\n",
    "        expls = expls[expls_order]\n",
    "        module_nums = module_nums[expls_order]\n",
    "\n",
    "        # sasc.viz.heatmap(s, expls, ylab=\"Voxel encoding model\",\n",
    "        #  xlab=\"Driving paragraph\", label_fontsize='large', clab='Mean model response ($\\sigma_f$)')\n",
    "        mats.append(s)\n",
    "\n",
    "    mat = np.array(mats).mean(axis=0)\n",
    "    mat_diffs = []\n",
    "    for i in range(mat.shape[0]):\n",
    "        mat_diffs.append(\n",
    "            mat[i][i] - (mat[i].sum() - mat[i][i]) / (mat.shape[0] - 1)\n",
    "        )\n",
    "    df = pd.DataFrame(mat_diffs, columns=['encoding_score'])\n",
    "    df['expl'] = expls\n",
    "    df['module_num'] = module_nums\n",
    "    df['subject'] = subject\n",
    "    dfs.append(df)\n",
    "    mat_dict[subject] = mat\n",
    "    expls_dict[subject] = expls\n",
    "joblib.dump(pd.concat(dfs), join(RESULTS_DIR,\n",
    "            'processed', 'data_ngram_scores.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expls_order = sasc.analyze_helper.sort_expls_semantically(expls)\n",
    "# expls_order = idx_order\n",
    "for subject in ['uts01', 'uts02', 'uts03']:\n",
    "    expls = expls_dict[subject]\n",
    "    mat = mat_dict[subject]\n",
    "    explanations = True\n",
    "\n",
    "    expls_order = [15, 7, 11, 14, 3, 12, 4, 1, 2, 0, 13, 6, 5, 16, 10, 9, 8]\n",
    "    s = mat[expls_order][:, expls_order]  # [:, expls_order]\n",
    "    sasc.viz.heatmap(\n",
    "        s, expls, ylab=\"Voxel explanation\",\n",
    "        xlab=\"Driving paragraph\", label_fontsize='large', clab='Mean model response ($\\sigma$)')\n",
    "    sasc.viz.outline_diagonal(s.shape, color='black', lw=1, block_size=1)\n",
    "    labs = [f'{i + 1:02d}' for i in range(len(expls))]\n",
    "    fontsize = 'medium'\n",
    "    if explanations:\n",
    "        expls = expls[expls_order]\n",
    "        plt.yticks(labels=expls, ticks=np.arange(\n",
    "            len(labs)), fontsize=fontsize)\n",
    "        plt.xticks(labels=expls, ticks=np.arange(\n",
    "            len(labs)), rotation=90, fontsize=fontsize)\n",
    "\n",
    "    else:\n",
    "        plt.yticks(labels=labs, ticks=np.arange(\n",
    "            len(labs)), fontsize=fontsize)\n",
    "        plt.xticks(labels=labs, ticks=np.arange(\n",
    "            len(labs)), rotation=90, fontsize=fontsize)\n",
    "\n",
    "    plt.savefig(join(RESULTS_DIR, 'figs', 'main',\n",
    "                f'data_score_heatmap_avg_{subject}.pdf'), bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module** <> Story Heatmap when varying ngram lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for EXPT_DIR in EXPT_DIRS:\n",
    "    # keys: scores_mod, scores_max_mod, all_scores, all_ngrams\n",
    "    # ngram_lengths = [10, 50, 100, 384]\n",
    "    ngram_lengths = [0]\n",
    "    for ngram_length in ngram_lengths:\n",
    "        scores_mod_dict = joblib.load(\n",
    "            join(EXPT_DIR, f'scores_mod_ngram_length={ngram_length}.pkl'))\n",
    "        # rows = joblib.load(join(EXPT_DIR, f'rows1_rep.pkl'))\n",
    "        rows = joblib.load(join(EXPT_DIR, f'rows.pkl'))\n",
    "        expls = rows.expl.values\n",
    "\n",
    "        s = scores_mod_dict['scores_mean'].T\n",
    "        # normalize each column\n",
    "        s = (s - s.mean(axis=0)) / s.std(axis=0)\n",
    "\n",
    "        if setting == 'default':\n",
    "            expls_order = sasc.analyze_helper.sort_expls_semantically(expls)\n",
    "            s = s[expls_order][:, expls_order]  # [:, expls_order]\n",
    "            expls = expls[expls_order]\n",
    "\n",
    "        sasc.viz.heatmap(s, expls, ylab='Story',\n",
    "                         xlab='Module (fit to fMRI voxel)', clab='Mean module response')\n",
    "\n",
    "        n = s.shape[0]\n",
    "        # plt.plot([0, n], [0, n], '--', color='gray', alpha=0.1)\n",
    "        # plt.title(f'Model window: {ngram_length} words', fontsize='small')\n",
    "        # plt.savefig(\n",
    "        # join(EXPT_DIR, f'mod_story_match_ngram_length={ngram_length}.png'), dpi=300)\n",
    "        plt.savefig(join(\n",
    "            EXPT_DIR, f'mod_story_match_ngram_length={ngram_length}.pdf'), bbox_inches='tight')\n",
    "        # plt.show()\n",
    "# sasc.viz.save_figs_to_single_pdf('../results/expl_mod_match.pdf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best voxels table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_diffs = defaultdict(list)\n",
    "for EXPT_DIR in EXPT_DIRS:\n",
    "    ngram_length = 0\n",
    "    scores_mod_dict = joblib.load(\n",
    "        join(EXPT_DIR, f'scores_mod_ngram_length={ngram_length}.pkl'))\n",
    "    rows = joblib.load(join(EXPT_DIR, f'rows.pkl'))\n",
    "    expls = rows.expl.values\n",
    "\n",
    "    s = scores_mod_dict['scores_mean'].T\n",
    "    s = scipy.special.softmax(s, axis=0)\n",
    "\n",
    "    BLOCK_SIZE = 4\n",
    "    for i in range(0, rows.shape[0]):\n",
    "        row = rows.iloc[i]\n",
    "        s_row = s[i]\n",
    "        block_idxs = range((i // BLOCK_SIZE) * BLOCK_SIZE,\n",
    "                           (i // BLOCK_SIZE + 1) * BLOCK_SIZE)\n",
    "        diff = s_row[i] - np.mean([s_row[j]\n",
    "                                  for j in range(s_row.shape[0]) if j not in block_idxs])\n",
    "        vox_diffs[(row.module_num, row.expl)].append(diff)\n",
    "vox_diffs = {k: np.mean(v) for k, v in vox_diffs.items()}\n",
    "vox_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all rows\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(pd.DataFrame(vox_diffs.items()).sort_values(\n",
    "        1, ascending=False).head(30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9ff692d44ea03fd8a03facee7621117bbbb82def09bacaacf0a2cbc238b7b91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
