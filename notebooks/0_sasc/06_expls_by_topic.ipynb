{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "import pandas as pd\n",
    "import joblib\n",
    "# from mprompt.config import RESULTS_DIR\n",
    "import torch.cuda\n",
    "import json\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import pandas as pd\n",
    "import re\n",
    "import sklearn.decomposition\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import word_tokenize\n",
    "import adjustText\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import joblib\n",
    "import os\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "import sasc.config\n",
    "\n",
    "RESULTS_DIR = join(sasc.config.REPO_DIR, 'results')\n",
    "# df = pd.read_csv(join(RESULTS_DIR, 'bert', \"dict_learn_results - extracted factors_all.csv\"))\n",
    "expls_scores_bert = joblib.load(join(RESULTS_DIR, 'bert', 'wiki_norm_std', 'wiki_our_syn_perc_std_score.pkl'))\n",
    "\n",
    "# BERT_RAW_RESULTS_DIR = '/home/chansingh/mntv1/aliyah/ah-module-prompt/results/'\n",
    "# for layer in tqdm(range(13)):\n",
    "#     layer_dir = f'dl_l{layer}'\n",
    "#     for factor in range(1500):\n",
    "#         factor_dir = f'i{factor}'\n",
    "#         extra_dir = os.listdir(join(BERT_RAW_RESULTS_DIR, layer_dir, factor_dir))[0]\n",
    "#         results = joblib.load(join(BERT_RAW_RESULTS_DIR, layer_dir, factor_dir, extra_dir, 'results.pkl'))\n",
    "#         expl = results['top_explanation_init_strs']\n",
    "#         expls.append(expl)\n",
    "# joblib.dump(join(RESULTS_DIR, 'bert', 'expls_bert.jbl'))\n",
    "expls_bert = joblib.load(join(RESULTS_DIR, 'bert', 'expls_bert.jbl'))\n",
    "\n",
    "\n",
    "# df_f = pd.read_pickle(join(RESULTS_DIR, 'results_fmri_full_old.pkl'))\n",
    "df_f = pd.read_pickle(join(RESULTS_DIR, 'results_fmri_full_1500_opt.pkl'))\n",
    "\n",
    "expls_fmri = df_f['top_explanation_init_strs'].values.tolist()\n",
    "\n",
    "df_bert = pd.DataFrame.from_dict({\n",
    "    'layer': sum([[i] * 1500 for i in range(13)], []),\n",
    "    'score': expls_scores_bert.flatten(),\n",
    "    'expl': expls_bert,\n",
    "})\n",
    "df_fmri = pd.DataFrame.from_dict({\n",
    "    'layer': 'fmri',\n",
    "    'score': df_f['top_score_synthetic'].values,\n",
    "    'expl': expls_fmri,\n",
    "})\n",
    "\n",
    "# filter dfs\n",
    "df_bert = df_bert[df_bert['score'] > 1]\n",
    "df_fmri = df_fmri[df_fmri['score'] > 0]\n",
    "n_bert = len(df_bert)\n",
    "expls = df_bert['expl'].values.tolist() + df_fmri['expl'].values.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: remove numbers, special characters, and lowercase all words\n",
    "processed_docs = [re.sub(r\"[^\\w\\s]\", \"\", doc.lower()) for doc in expls]\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", max_df=0.95, min_df=2)\n",
    "dtm = vectorizer.fit_transform(processed_docs)\n",
    "dtm_bert = dtm[:n_bert]\n",
    "\n",
    "# LDA model - only fitting to BERT!!!\n",
    "num_topics = 10\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Visualize clusters using PyLDAVis\n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.lda_model.prepare(lda, dtm, vectorizer, mds='tsne')\n",
    "# pyLDAvis.save_html(vis, join(RESULTS_DIR, 'bert', 'lda.html'))\n",
    "\n",
    "# add topics to df\n",
    "words = vectorizer.get_feature_names_out()\n",
    "topic_names_dict = {\n",
    "    topic_idx: \", \".join([words[i] for i in topic.argsort()[:-8:-1]])\n",
    "    for topic_idx, topic in enumerate(lda.components_)\n",
    "}\n",
    "print(topic_names_dict)\n",
    "topics = lda.transform(dtm).argmax(axis=1)\n",
    "df_bert[\"topic_num\"] = topics[:n_bert]\n",
    "df_fmri[\"topic_num\"] = topics[n_bert:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bert_lower = df_bert[:int(n_bert * (1/13))]\n",
    "# df_bert_upper = df_bert[int(n_bert * (12/13)):]\n",
    "# sns.set_style('ticks')\n",
    "counts = pd.merge(\n",
    "    (df_bert[\"topic_num\"].value_counts() / len(df_bert)).reset_index(),\n",
    "    (df_fmri[\"topic_num\"].value_counts() / len(df_fmri)).reset_index(),\n",
    "    # (df_bert_lower[\"topic_num\"].value_counts() / len(df_bert_lower)).reset_index(),\n",
    "    # (df_bert_upper[\"topic_num\"].value_counts() / len(df_bert_upper)).reset_index(),\n",
    "    on=\"topic_num\",\n",
    ").rename(columns={\"count_x\": \"bert\", \"count_y\": \"fmri\"})\n",
    "idxs = np.argsort(counts['bert'] - counts['fmri'])\n",
    "counts = counts.iloc[idxs]\n",
    "\n",
    "# make barplot from counts df with 'bert' and 'fmri' as the legend and topic_num as the ylabel\n",
    "H = 0.18\n",
    "eps = 0 #.005\n",
    "y = np.arange(len(counts))\n",
    "plt.barh(\n",
    "    y + H + eps,\n",
    "    counts[\"bert\"],\n",
    "    color=\"#6fa4c4\",\n",
    "    label=\"BERT\",\n",
    "    height=H * 2,\n",
    "    edgecolor=\"0.2\",\n",
    "    linewidth=1.5,\n",
    "    zorder=3,\n",
    ")\n",
    "plt.barh(\n",
    "    y - H - eps,\n",
    "    counts[\"fmri\"],\n",
    "    # color=(0.74073277, 0.38723941, 0.3813074),\n",
    "    # color= (0.9344867358708189, 0.2286812764321415, 0.17139561707035755),\n",
    "    # color = (0.9882352941176471, 0.732072279892349, 0.6299269511726259),\n",
    "    # color=\"#fc917c\",\n",
    "    color='#e57760',\n",
    "    label=\"fMRI\",\n",
    "    height=H * 2,\n",
    "    edgecolor=\"0.2\",\n",
    "    linewidth=1.5,\n",
    "    zorder=3,\n",
    ")\n",
    "plt.grid(zorder=0, axis='x')\n",
    "\n",
    "# plt.yticks(counts[\"topic_num\"], labels=counts[\"topic_num\"].map(topic_names_dict))\n",
    "plt.yticks(y, labels=counts[\"topic_num\"].map(topic_names_dict))\n",
    "plt.xlabel(\"Topic proportion\")\n",
    "plt.ylabel(\"Topic\")\n",
    "plt.legend()\n",
    "plt.savefig(\n",
    "    join(RESULTS_DIR, \"bert\", \"topic_proportions.png\"), dpi=300, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.savefig(\n",
    "    join(RESULTS_DIR, \"bert\", \"topic_proportions.pdf\"), dpi=300, bbox_inches=\"tight\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings\n",
    "embeddor = INSTRUCTOR('hkunlp/instructor-xl')\n",
    "instruction = \"Represent the short phrase for clustering:\"\n",
    "embs = embeddor.encode([[instruction, x_i] for x_i in expls], batch_size=32)\n",
    "# tsne = TSNE(n_components=2, random_state=42, verbose=False)\n",
    "# word_vectors_2d = tsne.fit_transform(embs)\n",
    "pca = sklearn.decomposition.PCA(n_components=2)\n",
    "word_vectors_2d = pca.fit_transform(embs)\n",
    "df_bert['x'] = word_vectors_2d[:, 0][:n_bert]\n",
    "df_bert['y'] = word_vectors_2d[:, 1][:n_bert]\n",
    "df_fmri['x'] = word_vectors_2d[:, 0][n_bert:]\n",
    "df_fmri['y'] = word_vectors_2d[:, 1][n_bert:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(df_bert, df_fmri, hue='layer', label_map_dict=None, adjust_text=True):\n",
    "    for lay in sorted(df_bert[hue].unique()):\n",
    "        df_plot_lay = df_bert[df_bert[hue] == lay]\n",
    "        if hue == 'layer':\n",
    "            label = f\"Layer {lay}\"\n",
    "        elif hue == 'topic_num':\n",
    "            label = label_map_dict[lay]\n",
    "        plt.plot(\n",
    "            df_plot_lay['x'],\n",
    "            df_plot_lay['y'],\n",
    "            marker=\".\",\n",
    "            alpha=0.25,\n",
    "            # markersize=1,\n",
    "            label=label,\n",
    "            color=sns.color_palette(\"viridis_r\", 26)[lay],\n",
    "            linestyle=\"None\",\n",
    "        )\n",
    "\n",
    "    plt.plot(\n",
    "            df_fmri['x'],\n",
    "            df_fmri['y'],\n",
    "            marker=\".\",\n",
    "            alpha=1,\n",
    "            # markersize=1,\n",
    "            label=f\"fMRI\",\n",
    "            color='pink',\n",
    "            linestyle=\"None\",\n",
    "        )\n",
    "    texts = []\n",
    "    for i, word in enumerate(df_bert['expl']):\n",
    "        if i % 50 == 0:\n",
    "            texts.append(\n",
    "                plt.annotate(word[:15], (word_vectors_2d[i, 0], word_vectors_2d[i, 1]), fontsize='xx-small'))\n",
    "    for i, word in enumerate(df_fmri['expl']):\n",
    "        if i % 50 == 0:\n",
    "            texts.append(\n",
    "                plt.annotate(word[:15], (word_vectors_2d[n_bert + i, 0], word_vectors_2d[n_bert + i, 1]), fontsize='xx-small'))\n",
    "\n",
    "    # make texts not overlap\n",
    "    if adjust_text:\n",
    "        print('adjusting texts...')\n",
    "        adjustText.adjust_text(texts) #, arrowprops=dict(arrowstyle='-', color='k', lw=0.5))\n",
    "\n",
    "    plt.xlabel(\"PC dimension 1\")\n",
    "    plt.ylabel(\"PC dimension 2\")\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "plot_tsne(df_bert, df_fmri, adjust_text=True)\n",
    "plt.legend(bbox_to_anchor=(1.1, 0.9))\n",
    "plt.savefig(join(RESULTS_DIR, 'bert', 'tsne_unrestricted.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9ff692d44ea03fd8a03facee7621117bbbb82def09bacaacf0a2cbc238b7b91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
