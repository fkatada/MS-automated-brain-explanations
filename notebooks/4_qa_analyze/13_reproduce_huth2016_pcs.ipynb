{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pprint import pprint\n",
    "from math import ceil\n",
    "import imodelsx.util\n",
    "import neuro.sasc.viz\n",
    "import pickle as pkl\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from numpy.linalg import norm\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from PIL import Image\n",
    "import img2pdf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import joblib\n",
    "from pprint import pprint\n",
    "from math import ceil\n",
    "import cortex\n",
    "from neuro import config\n",
    "from collections import defaultdict\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import gct\n",
    "from neuro.flatmaps_helper import load_flatmaps\n",
    "import neuro.sasc.viz\n",
    "from neuro import analyze_helper\n",
    "import nibabel as nib\n",
    "neurosynth_compare = __import__('04_neurosynth_compare')\n",
    "import neurosynth\n",
    "from neuro.features.questions.gpt4 import QS_35_STABLE\n",
    "import viz\n",
    "config.setup_freesurfer()\n",
    "import neuro.sasc.modules.fmri_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_35q_weights(subject):\n",
    "    d = defaultdict(list)\n",
    "    setting = 'individual_gpt4_ndel=1_pc_new'\n",
    "    flatmaps_qa_dict = joblib.load(\n",
    "        join(config.PROCESSED_DIR, subject.replace('UT', ''), setting + '.pkl'))\n",
    "    flatmaps_qa_list = [\n",
    "        flatmaps_qa_dict[q] for q in QS_35_STABLE\n",
    "    ]\n",
    "    weights_arr_to_cluster = np.vstack(flatmaps_qa_list)\n",
    "    return weights_arr_to_cluster\n",
    "\n",
    "\n",
    "def _get_llama_weights(subject):\n",
    "    mod = neuro.sasc.modules.fmri_module.fMRIModule(\n",
    "        subject=f\"UT{subject}\",\n",
    "        # checkpoint=\"facebook/opt-30b\",\n",
    "        checkpoint=\"huggyllama/llama-30b\",\n",
    "        init_model=False,\n",
    "        restrict_weights=False,\n",
    "    )\n",
    "    weights_arr_full = mod.weights\n",
    "    # weights_arr_full has shape (num_delays x num_linear_coefs, num_voxels)\n",
    "    weights_arr_to_cluster = weights_arr_full.reshape(\n",
    "        4, -1, weights_arr_full.shape[1]).mean(axis=0)\n",
    "    return weights_arr_to_cluster\n",
    "\n",
    "\n",
    "def compute_explained_variance(X, sparse_pca):\n",
    "    \"\"\"\n",
    "    Compute the explained variance of SparsePCA components.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy array): The original data matrix.\n",
    "    sparse_pca (SparsePCA object): A fitted SparsePCA model.\n",
    "\n",
    "    Returns:\n",
    "    explained_variance (numpy array): Variance explained by each component.\n",
    "    explained_variance_ratio (numpy array): Ratio of variance explained by each component.\n",
    "    \"\"\"\n",
    "    # Transform data using Sparse PCA\n",
    "    X_transformed = sparse_pca.transform(X)  # (n_samples, n_components)\n",
    "\n",
    "    # Compute variance explained by each component\n",
    "    n_samples = X.shape[0]\n",
    "    explained_variance = np.var(\n",
    "        X_transformed, axis=0) * (n_samples - 1) / n_samples\n",
    "\n",
    "    # Compute total variance of the original data\n",
    "    total_variance = np.var(X, axis=0).sum()\n",
    "\n",
    "    # Compute explained variance ratio\n",
    "    explained_variance_ratio = explained_variance / total_variance\n",
    "\n",
    "    return explained_variance, explained_variance_ratio\n",
    "\n",
    "\n",
    "subject = 'S03'\n",
    "# setting = 'llama'\n",
    "setting = '35q'\n",
    "# sparse = ''\n",
    "sparse = '_sparse'\n",
    "num_pcs = 4\n",
    "pc_dir = join('pca', subject)\n",
    "os.makedirs(pc_dir, exist_ok=True)\n",
    "\n",
    "SETTINGS = [\n",
    "    ('35q', '', None),\n",
    "    ('llama', '', None),\n",
    "    ('35q', '_sparse', 10),\n",
    "    ('35q', '_sparse', 20),\n",
    "    ('35q', '_sparse', 25),\n",
    "    ('35q', '_sparse', 30),\n",
    "    # ('35q', '_sparse', 100),\n",
    "]\n",
    "\n",
    "for setting, sparse, alpha in SETTINGS:\n",
    "    if setting == 'llama':\n",
    "        weights_arr_to_cluster = _get_llama_weights(subject)\n",
    "    elif setting == '35q':\n",
    "        weights_arr_to_cluster = _get_35q_weights(subject)\n",
    "\n",
    "    # pca\n",
    "    if sparse == '':\n",
    "        pca = PCA(n_components=num_pcs, whiten=True)\n",
    "    elif sparse == '_sparse':\n",
    "        pca = SparsePCA(n_components=num_pcs, alpha=alpha)\n",
    "    if alpha is None:\n",
    "        alpha_suffix = ''\n",
    "    else:\n",
    "        alpha_suffix = f'_alpha{alpha}'\n",
    "\n",
    "    X = weights_arr_to_cluster.T\n",
    "    X_normalized = normalize(X)\n",
    "    pca.fit(X_normalized)\n",
    "    if sparse == '_sparse':\n",
    "        explained_variance, explained_variance_ratio = compute_explained_variance(\n",
    "            X_normalized, pca)\n",
    "        pca.explained_variance_ = explained_variance\n",
    "        pca.explained_variance_ratio_ = explained_variance_ratio\n",
    "    print('frac zero', np.mean(pca.components_ == 0))\n",
    "    # pca.components_.shape is (num_pcs, num_linear_coefs)\n",
    "    # pc_coefs_per_voxel.shape is (num_voxels, num_pcs)\n",
    "    pc_coefs_per_voxel = pca.transform(X_normalized)\n",
    "\n",
    "    joblib.dump({'pc_coefs_per_voxel': pc_coefs_per_voxel,\n",
    "                'pca': pca}, join(pc_dir, f'pca{sparse}_{setting}{alpha_suffix}_coefs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_test = joblib.load(join(PROCESSED_DIR, subject.replace(\n",
    "    'UT', ''), 'corrs_test_35.pkl')).values[0]\n",
    "mask = corrs_test < 0.1\n",
    "\n",
    "\n",
    "fnames = sorted([join(pc_dir, f)\n",
    "                for f in os.listdir(pc_dir) if f.endswith('.pkl')])\n",
    "print(fnames)\n",
    "coefs_dict = {}\n",
    "for k in fnames:\n",
    "    coefs_dict[k] = joblib.load(k)\n",
    "\n",
    "\n",
    "# align sign of comps\n",
    "fname_gt = join(pc_dir, 'pca_llama_coefs.pkl')\n",
    "coef_gt = coefs_dict[fname_gt]['pc_coefs_per_voxel']\n",
    "\n",
    "# try to make flatmaps show positive regions\n",
    "k = fname_gt\n",
    "for pc_num in range(coef_gt.shape[1]):\n",
    "    avg_val = np.mean(coef_gt[:, pc_num][~mask])\n",
    "    if avg_val < 0:\n",
    "        coefs_dict[k]['pc_coefs_per_voxel'][:, pc_num] *= -1\n",
    "        coefs_dict[k]['pca'].components_[pc_num] *= -1\n",
    "\n",
    "# align other coefs to gt\n",
    "for k in coefs_dict:\n",
    "    corrs = []\n",
    "    for pc_num in range(coef_gt.shape[1]):\n",
    "        corr = np.corrcoef(\n",
    "            coefs_dict[k]['pc_coefs_per_voxel'][:, pc_num],\n",
    "            coefs_dict[fname_gt]['pc_coefs_per_voxel'][:, pc_num])[0, 1]\n",
    "        if corr < 0:\n",
    "            coefs_dict[k]['pc_coefs_per_voxel'][:, pc_num] *= -1\n",
    "            coefs_dict[k]['pca'].components_[pc_num] *= -1\n",
    "        corrs.append(corr)\n",
    "    # print(k, np.abs(corrs).round(2))\n",
    "\n",
    "# # visualize\n",
    "for k in tqdm(coefs_dict):\n",
    "    # save plots\n",
    "    for i in range(num_pcs):\n",
    "        flatmap = coefs_dict[k]['pc_coefs_per_voxel'][:, i]\n",
    "        flatmap[mask] = np.nan\n",
    "        neuro.sasc.viz.quickshow(\n",
    "            flatmap, subject, fname_save=join(pc_dir, f'pca_{i+1}.png'), kwargs={'with_curvature': True})\n",
    "\n",
    "    # read all plots and save as subplots on the same page\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 2.5))\n",
    "    axs = axs.ravel()\n",
    "    for i in range(num_pcs):\n",
    "        axs[i].imshow(Image.open(join(pc_dir, f'pca_{i+1}.png')))\n",
    "        axs[i].axis('off')\n",
    "        exp_var_ratio = coefs_dict[k]['pca'].explained_variance_ratio_[i]\n",
    "        axs[i].set_title(f'PC {i + 1} ({exp_var_ratio:.2f})')\n",
    "        # axs[i].set_title(f'PC {i + 1}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(k.replace('.pkl', '_subplots.png'), transparent=True)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = 'pca_35q_coefs.pkl'\n",
    "# fname = 'pca_sparse_35q_alpha20_coefs.pkl'\n",
    "fname = 'pca_sparse_35q_alpha25_coefs.pkl'\n",
    "# fname = 'pca_sparse_35q_alpha30_coefs.pkl'\n",
    "fname = join(pc_dir, fname)\n",
    "pcs = pd.DataFrame(coefs_dict[fname]['pca'].components_).T\n",
    "pcs.columns = [f'PC{i+1}' for i in range(pcs.shape[1])]\n",
    "pcs.index = QS_35_STABLE\n",
    "\n",
    "# drop rows that are all zero\n",
    "pcs = pcs.loc[(pcs != 0).any(axis=1)]\n",
    "\n",
    "# sort values by abs value of PC1 then PC2 then PC3 then PC4\n",
    "a = pcs.abs().sort_values(\n",
    "    by=['PC1', 'PC2', 'PC3', 'PC4'], ascending=False).index\n",
    "pcs = pcs.loc[a]\n",
    "sns.heatmap(pcs, cmap='coolwarm', center=0, mask=pcs == 0)\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# pcs = pcs.sort_values(\n",
    "#     by=['PC1', 'PC2', 'PC3', 'PC4'], ascending=False)\n",
    "# sns.heatmap(pcs, cmap='coolwarm', center=0, mask=pcs == 0)\n",
    "\n",
    "# sns.clustermap(pcs, col_cluster=False, cmap='coolwarm',\n",
    "#    center=0, mask=pcs == 0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(join(pc_dir, 'pc_loadings.png'), transparent=True,\n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d colormaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cortex\n",
    "import numpy as np\n",
    "\n",
    "# Load a sample subject\n",
    "subject = \"UTS02\"  # Change this to your actual subject ID\n",
    "xfmname = \"UTS02_auto\"  # Change based on your transformation\n",
    "\n",
    "# Generate three example maps (replace with real data)\n",
    "volume_shape = flatmap.shape\n",
    "map1 = pc_coefs_per_voxel[:, 0]\n",
    "map2 = pc_coefs_per_voxel[:, 1]\n",
    "map3 = pc_coefs_per_voxel[:, 2]\n",
    "\n",
    "# Normalize each map to be in range [0,1]\n",
    "map1 = (map1 - map1.min()) / (map1.max() - map1.min())\n",
    "map2 = (map2 - map2.min()) / (map2.max() - map2.min())\n",
    "map3 = (map3 - map3.min()) / (map3.max() - map3.min())\n",
    "\n",
    "# Create a 3-channel RGB volume\n",
    "rgb_volume = cortex.VolumeRGB(\n",
    "    map1, map2, map3, subject=subject, xfmname=xfmname)\n",
    "\n",
    "# Show the visualization\n",
    "cortex.quickshow(rgb_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
