{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import data\n",
    "import logging\n",
    "import pickle as pkl\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "from mprompt.config import RESULTS_DIR\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "def get_data(max_digit=1000):\n",
    "    d = defaultdict(list)\n",
    "    np.random.seed(13)\n",
    "    for num1 in trange(max_digit, desc=\"creating data\", leave=False):\n",
    "        for num2 in range(max_digit):\n",
    "            d['input'].append(f'{num1} {num2}')\n",
    "            d['output'].append(f' {num1 + num2}')\n",
    "    \"\"\"\n",
    "    for i in range(max_digit * max_digit):   \n",
    "        num1 = np.random.randint(0, max_digit)\n",
    "        num2 = np.random.randint(0, max_digit)\n",
    "        d['input'].append(f'{num1:03} {num2:03}')\n",
    "        d['output'].append(f' {num1 + num2:04}')\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame.from_dict(d)\n",
    "    # print(df.head())\n",
    "    df = df.sample(frac=1) # shuffle rows\n",
    "    # print(df.head())\n",
    "    dset = Dataset.from_pandas(df)\n",
    "    return dset\n",
    "\n",
    "def train(args, r, dset, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ------\n",
    "    r: dict\n",
    "        dictionary of things to save\n",
    "    \"\"\"\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    device = 'cuda'\n",
    "\n",
    "    model = model.to(device)\n",
    "    trans = model._modules['transformer']\n",
    "    wte = trans.wte.to(device)\n",
    "    dataloader = DataLoader(dset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # set up saving\n",
    "    save_dir_unique = datetime.now().strftime(\"%b_%d_%H_%M_\") + ''.join(random.choices(string.ascii_lowercase, k=12))\n",
    "    save_dir = os.path.join(args.save_dir, save_dir_unique)\n",
    "    logging.info('saving to ' + save_dir)\n",
    "\n",
    "    # initialize prefix\n",
    "    prefix_str = [\"x the following two numbers: \"]\n",
    "    prefix_inputs = tokenizer(prefix_str, return_tensors=\"pt\").to(device)\n",
    "    prefix_emb = wte.forward(prefix_inputs['input_ids'])\n",
    "    prefix_emb = torch.nn.Parameter(prefix_emb).to(device)\n",
    "\n",
    "    # optimizer\n",
    "    optim = torch.optim.Adam([prefix_emb], lr=args.lr)\n",
    "    for epoch in range(args.n_epochs):\n",
    "        for idx, batch in tqdm(enumerate(dataloader)):\n",
    "            x_text = batch['input']\n",
    "            y_text = batch['output']\n",
    "            full_text = [x_text[i] + y_text[i] for i in range(len(x_text))]\n",
    "            # print(full_text)\n",
    "            ex_inputs = tokenizer(full_text, return_tensors='pt').to(device)\n",
    "            # print(ex_inputs)\n",
    "            ex_embs = wte.forward(ex_inputs['input_ids'].to(\n",
    "                device)).to(device)\n",
    "\n",
    "            # concatenate prefix + example\n",
    "            emb = torch.cat((prefix_emb.repeat(ex_embs.shape[0], 1, 1),\n",
    "                            ex_embs), dim=1)\n",
    "\n",
    "            # go through model\n",
    "            outputs = model(inputs_embeds=emb)\n",
    "\n",
    "            # calculate loss\n",
    "            # currently this calculates loss only on the answer token\n",
    "            idxs_correct = tokenizer(y_text, return_tensors='pt')['input_ids'].to(device)\n",
    "            assert idxs_correct.nelement(\n",
    "            ) == args.batch_size, 'For now assume that answer is a single token'\n",
    "            # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "            last_token_logits = outputs['logits'][:, -1, :]\n",
    "            log_probs = torch.gather(last_token_logits, 1, idxs_correct)\n",
    "\n",
    "            # accumulate gradients in this batch\n",
    "            loss = -1 * log_probs.mean() # minimize prob answer being wrong\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "        # save stuff\n",
    "        r['embs'].append(prefix_emb.detach().cpu().numpy())\n",
    "        r['grads'].append(prefix_emb.grad.detach().cpu().numpy())\n",
    "        r['losses'].append(loss.item())\n",
    "        if epoch % args.epoch_save_interval == 0:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            pkl.dump(r, open(os.path.join(save_dir, 'results.pkl'), 'wb'))\n",
    "        # print('losses', loss)\n",
    "\n",
    "        # optimize\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "    return r\n",
    "\n",
    "def get_unembedding(checkpoint):\n",
    "    \"\"\"Get unembedding layer for first continuous vector\n",
    "    This is needed to take gradients wrt the input text\n",
    "    \"\"\"\n",
    "    checkpoint_clean = checkpoint.lower().replace('/', '___')\n",
    "    fname = join(SAVE_DIR, f'unembed_{checkpoint_clean}.pkl')\n",
    "    if os.path.exists(fname):\n",
    "        return pkl.load(open(fname, 'rb'))\n",
    "\n",
    "    # get the embedding from the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "    trans = model._modules['transformer']\n",
    "    w_embed = trans.wte.weight  # vocab_size, embed_dim\n",
    "    vocab_size = w_embed.shape[0]\n",
    "    embed_size = w_embed.shape[1]\n",
    "\n",
    "    # invert for unembedding\n",
    "    unemb_linear = nn.Linear(in_features=embed_size,\n",
    "                             out_features=vocab_size, bias=False)\n",
    "    pinv = torch.linalg.pinv(w_embed)\n",
    "    unemb_linear.weight = nn.Parameter(pinv.T)\n",
    "\n",
    "    pkl.dump(unemb_linear, open(fname, 'wb'))\n",
    "    return unemb_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:loading model and data...\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=1,\n",
    "                    help='batch size for training')\n",
    "parser.add_argument('--seed', type=int, default=1,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--n_epochs', type=int, default=10000,\n",
    "                    help='number of epochs for training')\n",
    "parser.add_argument('--max_digit', type=int, default=100,\n",
    "                    help='maximum value of each digit in summand')\n",
    "parser.add_argument('--save_dir', type=str, default='results',\n",
    "                    help='directory for saving')\n",
    "parser.add_argument('--epoch_save_interval', type=int, default=1,\n",
    "                    help='interval to save results')\n",
    "parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--checkpoint', type=str,\n",
    "                    # default=\"EleutherAI/gpt-neo-2.7B\",\n",
    "                    default=\"gpt2\",\n",
    "                    help='model checkpoint to use')\n",
    "args = parser.parse_args([])\n",
    "r = defaultdict(list)\n",
    "r.update(vars(args))\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "logger.info('loading model and data...')\n",
    "checkpoint = args.checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint, output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', '__index_level_0__'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = get_data(max_digit=args.max_digit)\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('beginning training...')\n",
    "r = train(args, r, dset, model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
