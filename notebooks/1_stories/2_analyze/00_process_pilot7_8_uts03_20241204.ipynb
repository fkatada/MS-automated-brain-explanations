{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import sasc.config\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sasc.modules.fmri_module import convert_module_num_to_voxel_num, add_stability_score\n",
    "from sasc.config import FMRI_DIR, STORIES_DIR\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "def extract_number(string):\n",
    "    return int(re.findall(r'\\d+', string)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read all the info from stories into a single pickle file\n",
    "Note: this session contained 2 different sets of explanations, which we save in separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stuff\n",
    "# PILOT7 #########################\n",
    "# output_file = join(sasc.config.RESULTS_DIR,\n",
    "#                    'processed', \"pilot7_story_data.pkl\")\n",
    "# story_mapping = {\n",
    "#     'roi/uts03___roi_nov30___seed=1_v1': 'GenStory36.npy',\n",
    "#     'roi/uts03___roi_nov30___seed=2_v1': 'GenStory38.npy',\n",
    "#     'roi/uts03___roi_nov30___seed=3_v1': 'GenStory40.npy',\n",
    "# }\n",
    "\n",
    "# PILOT8 #########################\n",
    "output_file = join(sasc.config.RESULTS_DIR,\n",
    "                   'processed', \"pilot8_story_data.pkl\")\n",
    "story_mapping = {\n",
    "    'roi/uts03___roi_nov30___seed=1_v2': 'GenStory37.npy',\n",
    "    'roi/uts03___roi_nov30___seed=2_v2': 'GenStory39.npy',\n",
    "    'roi/uts03___roi_nov30___seed=3_v2': 'GenStory41.npy',\n",
    "}\n",
    "\n",
    "STORY_DATA_DIR = join(FMRI_DIR, 'brain_tune', 'story_data', '20241204')\n",
    "\n",
    "\n",
    "# cluster_neighbors = joblib.load(join(FMRI_DIR, \"voxel_neighbors_and_pcs\", \"cluster_neighbors_v1.pkl\"))\n",
    "perfs = joblib.load(join(sasc.config.FMRI_DIR, 'sasc', 'rj_models',\n",
    "                    'opt_model', 'new_setup_performance.jbl'))\n",
    "\n",
    "# add keys\n",
    "stories_data_dict = defaultdict(list)\n",
    "for story_idx, story_name in enumerate(story_mapping.keys()):\n",
    "    # add scalar story descriptions\n",
    "    stories_data_dict[\"story_name_original\"].append(story_name)\n",
    "    stories_data_dict[\"story_setting\"].append(story_name.split(\"/\")[0])\n",
    "    stories_data_dict[\"story_name_new\"].append(story_mapping[story_name])\n",
    "    story_fname = [f for f in os.listdir(join(STORIES_DIR, story_name))\n",
    "                   if f.startswith('uts03_story')][0]\n",
    "    story_text = open(join(STORIES_DIR, story_name, story_fname), \"r\").read()\n",
    "    stories_data_dict[\"story_text\"].append(story_text)\n",
    "    # prompts_paragraphs = joblib.load(\n",
    "    # join(STORIES_DIR, story_name, \"prompts_paragraphs.pkl\")\n",
    "    # )\n",
    "    prompts = open(join(STORIES_DIR, story_name, \"prompts.txt\"),\n",
    "                   \"r\").read().split(\"\\n\\n\")\n",
    "    prompts = ['START_PARAGRAPH'] + prompts + ['END_PARAGRAPH']\n",
    "    prompts_paragraphs = {\n",
    "        'prompts': prompts,\n",
    "        'paragraphs': story_text.split(\"\\n\\n\"),\n",
    "    }\n",
    "    assert len(prompts_paragraphs['paragraphs']) == len(\n",
    "        prompts_paragraphs['prompts']), str(len(prompts_paragraphs['paragraphs'])) + ' ' + str(len(prompts_paragraphs['prompts']))\n",
    "\n",
    "    # add paragraph-level descriptions\n",
    "    timings = pd.read_csv(\n",
    "        # join(STORIES_DIR, story_name, \"timings_processed.csv\")\n",
    "        join(STORY_DATA_DIR,\n",
    "             f'timings_processed{extract_number(story_mapping[story_name])}.csv'),\n",
    "        header=None,\n",
    "    ).rename(columns={0: 'word', 2: 'time_running'})\n",
    "    timings['time_running'] *= 0.75\n",
    "    # = timings['time_running'].apply(\n",
    "    # lambda x: x * 0.75)\n",
    "    # add offset to timings\n",
    "    # timings['time_running']  # += 10  # -= 5  # += 6\n",
    "    stories_data_dict[\"timing\"].append(timings)\n",
    "    stories_data_dict[\"prompts\"].append(prompts_paragraphs[\"prompts\"])\n",
    "    stories_data_dict[\"paragraphs\"].append(prompts_paragraphs[\"paragraphs\"])\n",
    "\n",
    "    # add paragraph-level metadata\n",
    "    rows = pd.read_pickle(\n",
    "        join(STORIES_DIR, story_name, \"rows.pkl\"))\n",
    "    DUMMY_START = pd.DataFrame(\n",
    "        [{'expl': 'START', 'top_ngrams_module_correct': [],\n",
    "          'subject': 'UTS03', 'prompt_suffix': ''}])\n",
    "    DUMMY_END = pd.DataFrame(\n",
    "        [{'expl': 'END', 'top_ngrams_module_correct': [],\n",
    "          'subject': 'UTS03', 'prompt_suffix': ''}])\n",
    "    rows = pd.concat([DUMMY_START, rows, DUMMY_END])\n",
    "    stories_data_dict[\"rows\"].append(rows)\n",
    "\n",
    "joblib.dump(stories_data_dict, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in stories_data_dict:\n",
    "    print(k, len(stories_data_dict[k][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_data_dir = join(sasc.config.PILOT_STORY_DATA_DIR, '20241204')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load responses\n",
    "resp_np_files = [stories_data_dict['story_name_new'][i].replace(\n",
    "    '_resps', '') for i in range(len(stories_data_dict['story_name_new']))]\n",
    "resps_dict = {\n",
    "    k: np.load(join(pilot_data_dir, k))\n",
    "    for k in tqdm(resp_np_files)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(stories_data_dict[\"story_name_new\"])):\n",
    "    story_name = stories_data_dict[\"story_name_new\"][i]\n",
    "    timings = stories_data_dict[\"timing\"][i]\n",
    "    # trs = ceil(timings['time_running'].max() * 0.75 / 2) - 10\n",
    "    trs = ceil(timings['time_running'].max() / 2) - 10\n",
    "    print(story_name, trs, resps_dict[story_name].shape[0])\n",
    "\n",
    "    assert trs == resps_dict[story_name].shape[0]\n",
    "    # print('resp trs', resps_dict[story_name].shape[0])\n",
    "    # print(story_name.replace('_resps', ''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
