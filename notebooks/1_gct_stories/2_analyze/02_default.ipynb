{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "import joblib\n",
    "from scipy.special import softmax\n",
    "import neuro.sasc.config\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import neuro.sasc.viz\n",
    "from neuro.sasc import analyze_helper\n",
    "from neuro.sasc.modules.fmri_module import convert_module_num_to_voxel_num\n",
    "from neuro.sasc import config\n",
    "from scipy.stats import false_discovery_control\n",
    "import dvu\n",
    "dvu.set_style()\n",
    "\n",
    "# pcs = joblib.load(join(FMRI_DIR, \"voxel_neighbors_and_pcs\", \"loo_pc_UTS02.pkl\"))\n",
    "# pcs['good_voxels'].shape\n",
    "# pcs['pca_projections'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/chansingh/automated-brain-explanations/results/processed/pilot8_story_data.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pilot_name = 'pilot_story_data.pkl'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# pilot_name = 'pilot3_story_data.pkl'\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# pilot_name = 'pilot4_story_data.pkl'\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# pilot_name = \"pilot6_story_data.pkl\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m pilot_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpilot8_story_data.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m stories_data_dict \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRESULTS_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpilot_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pilot_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpilot_story_data.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     10\u001b[0m     pilot_data_dir \u001b[38;5;241m=\u001b[39m join(config\u001b[38;5;241m.\u001b[39mPILOT_STORY_DATA_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20230504\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.env/lib/python3.11/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/chansingh/automated-brain-explanations/results/processed/pilot8_story_data.pkl'"
     ]
    }
   ],
   "source": [
    "# pilot_name = 'pilot_story_data.pkl'\n",
    "# pilot_name = 'pilot3_story_data.pkl'\n",
    "# pilot_name = 'pilot4_story_data.pkl'\n",
    "# pilot_name = \"pilot6_story_data.pkl\"\n",
    "pilot_name = \"pilot8_story_data.pkl\"\n",
    "\n",
    "stories_data_dict = joblib.load(\n",
    "    join(config.RESULTS_DIR, 'processed', pilot_name))\n",
    "if pilot_name == 'pilot_story_data.pkl':\n",
    "    pilot_data_dir = join(config.PILOT_STORY_DATA_DIR, '20230504')\n",
    "elif pilot_name == 'pilot3_story_data.pkl':\n",
    "    pilot_data_dir = join(config.PILOT_STORY_DATA_DIR, '20231106')\n",
    "elif pilot_name == 'pilot4_story_data.pkl':\n",
    "    pilot_data_dir = join(config.PILOT_STORY_DATA_DIR, '20240509')\n",
    "elif pilot_name == 'pilot6_story_data.pkl':\n",
    "    pilot_data_dir = join(config.PILOT_STORY_DATA_DIR, '20241202')\n",
    "elif pilot_name == 'pilot7_story_data.pkl':\n",
    "    pilot_data_dir = join(config.PILOT_STORY_DATA_DIR, '20241204')\n",
    "elif pilot_name == 'pilot8_story_data.pkl':\n",
    "    pilot_data_dir = join(config.PILOT_STORY_DATA_DIR, '20241204')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load responses\n",
    "default_story_idxs = np.where(\n",
    "    (np.array(stories_data_dict['story_setting']) == 'default') |\n",
    "    (np.array(stories_data_dict['story_setting']) == 'roi')\n",
    ")[0]\n",
    "resp_np_files = [stories_data_dict['story_name_new'][i].replace('_resps', '')\n",
    "                 for i in default_story_idxs]\n",
    "resps_dict = {\n",
    "    k: np.load(join(pilot_data_dir, k))\n",
    "    for k in tqdm(resp_np_files)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats = defaultdict(list)\n",
    "resp_chunks = defaultdict(list)\n",
    "if pilot_name in ['pilot1']:\n",
    "    use_clusters_list = [False, True]\n",
    "else:\n",
    "    use_clusters_list = [False]\n",
    "for use_clusters in use_clusters_list:\n",
    "    for story_num in default_story_idxs:\n",
    "        rows = stories_data_dict[\"rows\"][story_num]\n",
    "\n",
    "        # get resp_chunks\n",
    "        resp_story = resps_dict[\n",
    "            stories_data_dict[\"story_name_new\"][story_num].replace(\n",
    "                '_resps', '')\n",
    "        ].T  # (voxels, time)\n",
    "        timing = stories_data_dict[\"timing\"][story_num]\n",
    "        if 'paragraphs' in stories_data_dict.keys():\n",
    "            paragraphs = stories_data_dict[\"paragraphs\"][story_num]\n",
    "        else:\n",
    "            paragraphs = stories_data_dict[\"story_text\"][story_num].split(\n",
    "                \"\\n\\n\")\n",
    "        # paragraphs = stories_data_dict[\"story_text\"][story_num].split(\"\\n\\n\")\n",
    "        if pilot_name in ['pilot3_story_data.pkl']:\n",
    "            paragraphs = [neuro.sasc.analyze_helper.remove_repeated_words(\n",
    "                p) for p in paragraphs]\n",
    "        assert len(paragraphs) == len(\n",
    "            rows), f\"{len(paragraphs)} != {len(rows)}\"\n",
    "        resp_chunks = analyze_helper.get_resps_for_paragraphs(\n",
    "            timing, paragraphs, resp_story, offset=2, validate=True,\n",
    "            split_hyphens=pilot_name in [\"pilot6_story_data.pkl\", \"pilot7_story_data.pkl\", \"pilot8_story_data.pkl\"])\n",
    "        assert len(resp_chunks) <= len(paragraphs)\n",
    "\n",
    "        # calculate mat\n",
    "        mat = np.zeros((len(rows), len(paragraphs)))\n",
    "        for i in range(len(resp_chunks)):\n",
    "            if use_clusters == False:\n",
    "                # driving single voxel\n",
    "                if 'voxel_num' in rows.columns:\n",
    "                    mat[:, i] = resp_chunks[i][rows[\"voxel_num\"].values].mean(\n",
    "                        axis=1).flatten()\n",
    "                elif 'voxel_nums' in rows.columns:\n",
    "                    mat[:, i] = [resp_chunks[i][x].mean()\n",
    "                                 for x in rows['voxel_nums']]\n",
    "                    # resp_chunks[i][rows[\"voxel_nums\"].values].mean(\n",
    "                    # axis=1).flatten()\n",
    "\n",
    "            elif use_clusters == True:\n",
    "                for r in range(len(rows)):\n",
    "                    cluster_nums = rows.iloc[r][\"cluster_nums\"]\n",
    "                    if isinstance(cluster_nums, np.ndarray):\n",
    "                        vals = resp_chunks[i][cluster_nums].flatten()\n",
    "                        mat[r, i] = np.nanmean(vals)\n",
    "                    else:\n",
    "                        # print(cluster_nums)\n",
    "                        mat[r, i] = np.nan\n",
    "        mat[:, 0] = np.nan  # ignore the first column\n",
    "        # print('mat', mat)\n",
    "\n",
    "        # sort by voxel_num\n",
    "        if 'voxel_num' in rows.columns:\n",
    "            args = np.argsort(rows[\"voxel_num\"].values)\n",
    "        elif pilot_name in [\"pilot6_story_data.pkl\", \"pilot7_story_data.pkl\", \"pilot8_story_data.pkl\"]:\n",
    "            args = np.argsort(rows[\"expl\"].values)\n",
    "        else:\n",
    "            args = np.argsort(rows[\"roi\"].values)\n",
    "        mat = mat[args, :][:, args]\n",
    "        mats[use_clusters].append(deepcopy(mat))\n",
    "\n",
    "        # plt.imshow(mat)\n",
    "        # plt.colorbar(label=\"Mean response\")\n",
    "        # plt.xlabel(\"Corresponding paragraph\\n(Ideally, diagonal should be brighter)\")\n",
    "        # plt.ylabel(\"Voxel\")\n",
    "        # plt.title(f\"{story_data['story_name_new'][story_num][3:-10]}\")\n",
    "        # plt.show()\n",
    "\n",
    "if 'voxel_num' in rows.columns:\n",
    "    rows = rows.sort_values(by=\"voxel_num\")\n",
    "elif pilot_name in [\"pilot6_story_data.pkl\", \"pilot7_story_data.pkl\", \"pilot8_story_data.pkl\"]:\n",
    "    rows = rows.sort_values(by=\"expl\")\n",
    "else:\n",
    "    rows = rows.sort_values(by=\"roi\")\n",
    "expls = rows[\"expl\"].values\n",
    "\n",
    "\n",
    "m = {}\n",
    "for use_clusters in [False, True]:\n",
    "    mats[use_clusters] = np.array(mats[use_clusters])  # (6, 17, 17)\n",
    "    m[use_clusters] = np.nanmean(mats[use_clusters], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make average plot\n",
    "# calculate means\n",
    "use_clusters = False\n",
    "m1 = m[use_clusters]\n",
    "diag_means = np.diag(m1)\n",
    "off_diag_means = np.nanmean(m1, axis=1) - (diag_means / len(diag_means))\n",
    "neuro.sasc.viz.barplot_default([diag_means], [off_diag_means],\n",
    "                               pilot_name, expls, annot_points=False)\n",
    "joblib.dump({'diag_means': diag_means,\n",
    "            'off_diag_means': off_diag_means}, join(config.RESULTS_DIR, 'processed', pilot_name.replace('_story_data.pkl', '_default_means.pkl')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between different voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: some voxels didn't have good clusters so they will be missing from these plots...\n",
    "use_clusters = False\n",
    "m1 = m[use_clusters]\n",
    "\n",
    "neuro.sasc.viz.outline_diagonal(m1.shape, color='black', lw=1, block_size=1)\n",
    "\n",
    "s = 'small'\n",
    "# expls_order = analyze_helper.sort_expls_semantically(expls, device='cuda')\n",
    "expls_order = np.argsort(expls)\n",
    "if pilot_name == 'pilot_story_data.pkl':\n",
    "    expls_order = expls_order[[15, 7, 11, 14, 3,\n",
    "                               12, 4, 1, 2, 0, 13, 6, 5, 16, 10, 9, 8]]\n",
    "m_plot = m1[expls_order][:, expls_order]  # [:, expls_order]\n",
    "neuro.sasc.viz.imshow_diverging(\n",
    "    m_plot, clab=\"Mean response ($\\sigma$)\", clab_size='large')\n",
    "plt.xlabel(\"Driving paragraph\",  # \\n(Ideally, diagonal should be brighter)\",\n",
    "           fontsize='large')\n",
    "\n",
    "# plt.ylabel(\"Voxel\", fontsize='x-small')\n",
    "# labs = expls[expls_order]\n",
    "\n",
    "plt.ylabel(\"Voxel number\", fontsize='large')\n",
    "labs = [f'{i + 1:02d}' for i in range(len(expls_order))]\n",
    "for i in range(len(labs)):\n",
    "    print(labs[i], expls[expls_order[i]])\n",
    "\n",
    "plt.yticks(labels=labs, ticks=np.arange(\n",
    "    len(expls)), fontsize=s)\n",
    "plt.xticks(labels=labs, ticks=np.arange(\n",
    "    len(expls)), rotation=90, fontsize=s)\n",
    "plt.tight_layout()\n",
    "plt.savefig(join(config.RESULTS_DIR, 'figs/main',\n",
    "            pilot_name[:pilot_name.index('_')] + '_default_heatmap.pdf'), bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# plot correlations across all resps\n",
    "# resps_voxels = np.concatenate(\n",
    "#     [resps_dict[story_data[\"story_name_new\"][story_num]].T for story_num in [2, 3, 4]],\n",
    "#     axis=1,\n",
    "# )[rw[\"voxel_num\"].values]\n",
    "# corr = pd.DataFrame(resps_voxels.T, columns=expls).corr().round(2)\n",
    "# sns.clustermap(corr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Story-level differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_story_df(mats1, melt=False):\n",
    "    d = defaultdict(list)\n",
    "    story_names = resp_np_files\n",
    "    for i in range(len(mats1)):\n",
    "        m = mats1[i]\n",
    "        d['driving'].append(np.nanmean(np.diag(m)))\n",
    "        d['baseline'].append(np.nanmean(m[~np.eye(m.shape[0], dtype=bool)]))\n",
    "        d['story'].append(story_names[i].replace('.npy', ''))\n",
    "    d = pd.DataFrame.from_dict(d)\n",
    "    if melt:\n",
    "        d = d.melt(id_vars='story', value_vars=[\n",
    "            'driving', 'baseline'], var_name='condition', value_name='mean')\n",
    "        d = d[d.condition == 'driving']\n",
    "    return d\n",
    "\n",
    "\n",
    "use_clusters = False\n",
    "mats1 = mats[use_clusters]\n",
    "story_scores_df = get_story_df(mats1)\n",
    "joblib.dump(story_scores_df, join(config.RESULTS_DIR, 'processed',\n",
    "            pilot_name.replace('_story_data.pkl', '_default_story_scores.pkl')))\n",
    "\n",
    "neuro.sasc.viz.stories_barplot(story_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test per subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.shape\n",
    "diags = np.diag(m1)\n",
    "off_diags = m1[~np.eye(m1.shape[0], dtype=bool)]\n",
    "\n",
    "n_permutations = 10000\n",
    "n_pick = len(diags)\n",
    "baseline_distr = []\n",
    "rng = np.random.default_rng(42)\n",
    "for i in range(n_permutations):\n",
    "    sample = rng.choice(\n",
    "        off_diags, n_pick, replace=False)\n",
    "    baseline_distr.append(np.nanmean(sample))\n",
    "baseline_distr = np.array(baseline_distr)\n",
    "p = np.mean(baseline_distr > np.nanmean(diags))\n",
    "print(pilot_name, p)\n",
    "PS = {\n",
    "    'UTS01': 0.0202,\n",
    "    'UTS02': 0,\n",
    "    'UTS03': 0.0057,\n",
    "}\n",
    "false_discovery_control(list(PS.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test per voxel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_stories, n_voxels, driving_paragraph\n",
    "diag_means2 = []\n",
    "mats_list = mats[use_clusters]\n",
    "ps_per_voxel = []\n",
    "for vox_num in range(mats_list.shape[1]):\n",
    "    driving_paragraph_means = mats_list[:, vox_num, vox_num]\n",
    "    non_driving_paragraph_means = np.delete(\n",
    "        mats_list[:, vox_num, :], vox_num, axis=-1).flatten()\n",
    "\n",
    "    # permutation test per voxel\n",
    "    n_permutations = 1000\n",
    "    n_pick = len(driving_paragraph_means)\n",
    "    baseline_distr = []\n",
    "    rng = np.random.default_rng(42)\n",
    "    for i in range(n_permutations):\n",
    "        sample = rng.choice(\n",
    "            non_driving_paragraph_means, n_pick, replace=False)\n",
    "        baseline_distr.append(np.nanmean(sample))\n",
    "    baseline_distr = np.array(baseline_distr)\n",
    "    p = np.mean(baseline_distr > np.nanmean(driving_paragraph_means))\n",
    "    ps_per_voxel.append(p)\n",
    "\n",
    "    # note: actual min should be 1 / num_possible_permutations\n",
    "print(ps_per_voxel)\n",
    "PS_PER_VOXEL_OUTPUT = {\n",
    "    'UTS01': [0.497, 0.123, 0.304, 0.113, 0.157, 0.478, 0.836, 0.072, 0.273, 0.325, 0.426, 0.251, 0.626, 0.04, 0.109, 0.664, 0.704],\n",
    "    'UTS02': [0.771, 0.835, 0.07, 0.113, 0.0, 0.167, 0.037, 0.112, 0.018, 0.351, 0.005, 0.0, 0.0, 0.0, 0.019, 0.775, 0.006],\n",
    "    'UTS03': [0.003, 0.485, 0.491, 0.074, 0.103, 0.318, 0.961, 0.103, 0.218, 0.182, 0.057, 0.282, 0.392, 0.699, 0.287, 0.37, 0.201],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra analysis for UTS02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters vs non-clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.color_palette(\"Blues\", 2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pilot_name == 'pilot_story_data.pkl'\n",
    "df1 = get_story_df(mats[False], melt=True)\n",
    "df1['Setting'] = 'Single voxel'\n",
    "df2 = get_story_df(mats[True], melt=True)\n",
    "df2['Setting'] = 'Voxel cluster'\n",
    "df = pd.concat([df1, df2])\n",
    "df['story'] = df['story'].str.replace('GenStory', '')\n",
    "\n",
    "sns.barplot(data=df, x='story', y='mean',\n",
    "            hue='Setting',\n",
    "            width=0.8,\n",
    "            palette=[sns.color_palette(\"Blues\", 3)[0], 'lightgray'],\n",
    "            )\n",
    "plt.xlabel(\"Story\")\n",
    "plt.ylabel('Mean driving voxel response ($\\sigma$)')\n",
    "plt.savefig(join(config.RESULTS_DIR, 'figs/misc',\n",
    "            'cluster_vs_single_default_story_breakdown.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good prompt vs bad prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pilot_name == 'pilot_story_data.pkl'\n",
    "df = get_story_df(mats[False], melt=True)\n",
    "good_prompt = ['GenStory2', 'GenStory3', 'GenStory4']\n",
    "df['Prompt'] = df.apply(lambda x: x['story'] in good_prompt, axis=1)\n",
    "df['Prompt'] = df['Prompt'].map(\n",
    "    {True: 'Prompt version 1', False: 'Prompt version 0'})\n",
    "df = df.sort_values(by='Prompt', ascending=False)\n",
    "df['story'] = df['story'].str.replace('GenStory', '')\n",
    "\n",
    "\n",
    "# shade bars by prompt version\n",
    "offset = 0\n",
    "xticklabels = []\n",
    "for i, prompt in enumerate(df['Prompt'].unique()):\n",
    "    d = df[df['Prompt'] == prompt]\n",
    "    d = d.sort_values('story')\n",
    "    plt.bar(np.arange(len(d)) + offset, d['mean'], label=prompt,\n",
    "            color=sns.color_palette(\"Blues\", 3)[0], hatch='' if i == 0 else '//')\n",
    "    xticklabels += d['story'].tolist()\n",
    "    offset += len(d)\n",
    "plt.xticks(np.arange(len(xticklabels)), xticklabels)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Story\")\n",
    "plt.ylabel('Mean driving voxel response ($\\sigma$)')\n",
    "plt.savefig(join(config.RESULTS_DIR, 'figs/misc',\n",
    "            'prompt_default_story_breakdown.pdf'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxel-level differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add stats\n",
    "def save_voxel_scores(m, rows, pilot_name):\n",
    "    rows[\"driving_score\"] = np.diag(m[False])\n",
    "    rm = pd.read_pickle(join(config.RESULTS_DIR, 'neuro.sasc', \"fmri_results_merged.pkl\")).sort_values(\n",
    "        by=[\"stability_score\"], ascending=False\n",
    "    )\n",
    "    for k in [\"fmri_test_corr_llama\", \"top_score_normalized_llama\"]:\n",
    "        rows[k] = rows.apply(\n",
    "            lambda row: rm[\n",
    "                (rm.module_num == row.module_num) & (rm.subject == row.subject)\n",
    "            ].iloc[0][k],\n",
    "            axis=1,\n",
    "        ).values\n",
    "    cols = [\"expl\", \"driving_score\", \"stability_score\", \"top_score_normalized\", \"top_score_normalized_llama\",\n",
    "            \"fmri_test_corr\", \"fmri_test_corr_llama\", \"module_num\"]\n",
    "    voxel_scores = rows[cols]\n",
    "    joblib.dump(voxel_scores, join(config.RESULTS_DIR, 'processed',\n",
    "                pilot_name.replace('_story_data.pkl', '_default_voxel_scores.pkl')))\n",
    "\n",
    "\n",
    "save_voxel_scores(m, rows, pilot_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(rows[cols + [\"expl\"]], kind=\"reg\")  # , hue='expl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
