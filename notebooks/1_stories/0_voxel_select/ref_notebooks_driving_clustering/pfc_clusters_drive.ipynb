{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pprint import pprint\n",
    "from math import ceil\n",
    "import imodelsx.util\n",
    "import sasc.viz\n",
    "import pickle as pkl\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from numpy.linalg import norm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from PIL import Image\n",
    "import img2pdf\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sasc.config import CACHE_DIR, RESULTS_DIR, cache_ngrams_dir, regions_idxs_dir, FMRI_DIR, SAVE_DIR_FMRI\n",
    "import sasc.modules.fmri_module\n",
    "from imodels import StableClustering\n",
    "ngrams_list = joblib.load(join(cache_ngrams_dir, 'fmri_UTS02_ngrams.pkl')) # ngrams are same for both models\n",
    "\n",
    "subject = 'S02'\n",
    "k_values = np.unique(np.logspace(0.5, 3, 50, dtype=int))\n",
    "# subject = 'S03'\n",
    "# rois_dict = joblib.load(join(regions_idxs_dir, f'rois_{subject}.jbl'))\n",
    "# sorted(rois_dict.keys())\n",
    "# if subject == 'S03':\n",
    "#     rois_dict['OPA'] = rois_dict['TOS']\n",
    "SUBJECT_VOX_COUNTS_DICT = {\n",
    "    'S02': 94251,\n",
    "}\n",
    "# suffix_setting = ''\n",
    "suffix_setting = '_filt=0.15'\n",
    "# suffix_setting = '_rj_custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pfc stuff\n",
    "pfc = joblib.load(join(FMRI_DIR, 'brain_tune/voxel_neighbors_and_pcs/',\n",
    "                  'PFC_spotlights.jbl')).astype(bool)\n",
    "data = np.load(\n",
    "    join(FMRI_DIR, 'brain_tune/voxel_neighbors_and_pcs/', 'connectivity.npz'))\n",
    "connectivity = csr_matrix(\n",
    "    (data['data'], data['indices'], data['indptr']), shape=data['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sasc.modules.fmri_module.fMRIModule(\n",
    "    subject=f\"UT{subject}\",\n",
    "    # checkpoint=\"facebook/opt-30b\",\n",
    "    checkpoint=\"huggyllama/llama-30b\",\n",
    "    init_model=False,\n",
    "    restrict_weights=False,\n",
    ")\n",
    "weights_arr_full = mod.weights\n",
    "# weights_arr_full has shape (num_delays x num_linear_coefs, num_voxels)\n",
    "\n",
    "if suffix_setting == '_filt=0.15':\n",
    "    print(pfc.sum())\n",
    "    pfc[mod.corrs < 0.15] = False\n",
    "    print('filtered to ', pfc.sum())\n",
    "\n",
    "    # histogram, with dashes for percentiles\n",
    "    # plt.hist(mod.corrs[pfc])\n",
    "    # for q in np.arange(10, 100, 10):\n",
    "    #     perc = np.percentile(mod.corrs[pfc], q)\n",
    "    #     plt.axvline(perc, linestyle='--', color='k')\n",
    "\n",
    "    #     # annotate with percentile and value\n",
    "    #     plt.text(np.percentile(mod.corrs[pfc], q),\n",
    "    #              0, f'{q}th ({perc:0.3f})', rotation=90, va='bottom')\n",
    "    # plt.xlabel('Test corr')\n",
    "\n",
    "# mean over delays\n",
    "weights_arr_to_cluster = weights_arr_full.reshape(\n",
    "    4, -1, weights_arr_full.shape[1]).mean(axis=0)\n",
    "# weights_arr_to_cluster.shape is (num_linear_coefs, num_voxels)\n",
    "\n",
    "# only keep PFC voxels\n",
    "weights_arr_to_cluster_pfc = weights_arr_to_cluster[:, pfc]\n",
    "# weights_arr_to_cluster_pfc.shape is (num_linear_coefs, num_pfc_voxels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca\n",
    "num_pcs = 20\n",
    "pca = PCA(n_components=num_pcs, whiten=True)\n",
    "\n",
    "pca.fit(normalize(weights_arr_to_cluster_pfc.T))\n",
    "# pca.fit(normalize(weights_arr_to_cluster.T))\n",
    "# pca.components_.shape is (num_pcs, num_linear_coefs)\n",
    "\n",
    "pc_coefs_per_voxel = pca.transform(normalize(weights_arr_to_cluster.T))\n",
    "# pc_coefs_per_voxel.shape is (num_voxels, num_pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save pcs\n",
    "# for pc_num in range(num_pcs):\n",
    "#     flatmap = pc_coefs_per_voxel[:, pc_num]\n",
    "#     # flatmap = np.zeros(weights_arr_full.shape[1])\n",
    "#     # flatmap[pfc] = pc_coefs_per_voxel[:, pc_num]\n",
    "#     sasc.viz._save_flatmap(\n",
    "#         flatmap, 'S02', fname_save=f'pca_images/pca_{pc_num+1}.png')\n",
    "\n",
    "# # read all plots and save as subplots on the same page\n",
    "# fig, axs = plt.subplots(ceil(num_pcs/3), 3, figsize=(15, 5))\n",
    "# axs = axs.ravel()\n",
    "# for i in range(num_pcs):\n",
    "#     axs[i].imshow(Image.open(f'./pca_{i+1}.png'))\n",
    "#     axs[i].axis('off')\n",
    "#     axs[i].set_title(f'PC {i + 1}')\n",
    "# plt.savefig('pca_subplots.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_coefs_per_pfc_voxel = pc_coefs_per_voxel[pfc]\n",
    "connectivity_pfc = connectivity[pfc][:, pfc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if suffix_setting == '_rj_custom':\n",
    "    clusters = joblib.load('all_new_rois_UTS02_gemv_semmod.jbl')\n",
    "    cluster_arr = np.zeros_like(clusters[0])\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        cluster_arr[cluster.astype(bool)] = i + 1\n",
    "    # decrease all vals by 1 (to avoid the \"0\" cluster)\n",
    "    cluster_arr -= 1\n",
    "\n",
    "    cluster_assignments_dict = {len(clusters): cluster_arr}\n",
    "    k_values = [len(clusters)]\n",
    "\n",
    "else:\n",
    "    cluster_assignments_dict = {}\n",
    "    for num_clusters in tqdm(k_values):\n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=num_clusters,\n",
    "            connectivity=connectivity_pfc,\n",
    "            linkage='ward',\n",
    "        )\n",
    "\n",
    "        clustering.fit(pc_coefs_per_pfc_voxel)\n",
    "        cluster_assignments = clustering.labels_\n",
    "        cluster_assignments_dict[num_clusters] = deepcopy(cluster_assignments)\n",
    "\n",
    "        # visualize clusters\n",
    "        for i in range(min(20, num_clusters)):\n",
    "            # flatmap = pc_coefs_per_voxel[:, pc_num]\n",
    "            flatmap = np.zeros(weights_arr_full.shape[1])\n",
    "            flatmap[pfc] = (cluster_assignments == i).astype(int)\n",
    "            sasc.viz._save_flatmap(\n",
    "                flatmap, 'S02', fname_save=f'clusters/{i+1}.png')\n",
    "\n",
    "        # # read all plots and save as subplots on the same page\n",
    "        # C = 4\n",
    "        # R = ceil(min(20, num_clusters)/C)\n",
    "        # fig, axs = plt.subplots(R, C, figsize=(C * 4, R * 2))\n",
    "        # axs = axs.ravel()\n",
    "        # for i in range(min(20, num_clusters)):\n",
    "        #     axs[i].imshow(Image.open(f'clusters/{i+1}.png'))\n",
    "        #     axs[i].axis('off')\n",
    "        #     axs[i].set_title(f'{i + 1}')\n",
    "        # plt.savefig(f'clusters/cluster_aggregated_{num_clusters}.png')\n",
    "    joblib.dump(cluster_assignments_dict, join(\n",
    "        cache_ngrams_dir, f'cluster_assignments_dict_{subject}{suffix_setting}.jbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_assignments_dict = joblib.load(join(\n",
    "#     cache_ngrams_dir, f'cluster_assignments_dict_{subject}{suffix_setting}.jbl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use clustering to get predictions etc.\n",
    "(This is the slowest part, involves a massive matrix multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embs_fname, checkpoint, out_suffix in zip(\n",
    "    ['fmri_embs.pkl', 'fmri_embs_llama.pkl'],\n",
    "    ['facebook/opt-30b', 'huggyllama/llama-30b'],\n",
    "    ['_opt', '_llama'],\n",
    "):\n",
    "    print(f'Running for {embs_fname}')\n",
    "    embs = joblib.load(join(cache_ngrams_dir, embs_fname))\n",
    "    # embs = joblib.load(join(cache_ngrams_dir, 'fmri_embs_llama.pkl'))\n",
    "    mod = sasc.modules.fmri_module.fMRIModule(\n",
    "        subject=f\"UT{subject}\",\n",
    "        checkpoint=checkpoint,\n",
    "        init_model=False,\n",
    "        restrict_weights=False,\n",
    "    )\n",
    "    voxel_preds = mod(embs=embs, return_all=True)\n",
    "\n",
    "    print('Saving outputs for clusters')\n",
    "    for k in tqdm(k_values):\n",
    "        cluster_assignments_arr = cluster_assignments_dict[k]\n",
    "        outputs_dict = {\n",
    "            k_: np.nanmean(voxel_preds[:,\n",
    "                                       np.where(\n",
    "                                           cluster_assignments_arr == k_)[0]\n",
    "                                       ], axis=1)\n",
    "            for k_ in range(k)\n",
    "        }\n",
    "        joblib.dump(outputs_dict, join(\n",
    "            cache_ngrams_dir, f'pfc_clusters_ngram_outputs_dict_{k}_{subject}{out_suffix}{suffix_setting}.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarities between clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nancorr(x, y):\n",
    "    # remove nans\n",
    "    mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "    return np.corrcoef(x[mask], y[mask])[0, 1]\n",
    "\n",
    "\n",
    "cluster_corrs_list = []\n",
    "for k in tqdm(k_values):\n",
    "    # for k in [3]:\n",
    "    outputs_dict_opt = joblib.load(join(\n",
    "        cache_ngrams_dir, f'pfc_clusters_ngram_outputs_dict_{k}_{subject}_opt{suffix_setting}.pkl'))\n",
    "    outputs_dict_llama = joblib.load(join(\n",
    "        cache_ngrams_dir, f'pfc_clusters_ngram_outputs_dict_{k}_{subject}_llama{suffix_setting}.pkl'))\n",
    "\n",
    "    cluster_corrs = [nancorr(outputs_dict_opt[k], outputs_dict_llama[k])\n",
    "                     for k in outputs_dict_opt]\n",
    "    cluster_corrs_list.append(cluster_corrs)\n",
    "    # print(\n",
    "    # f'k = {k}: {np.nanmean(cluster_corrs):.2f} +/- {np.nanstd(cluster_corrs):.2f}')\n",
    "# plt.plot(k_values, [np.nanmean(x) for x in cluster_corrs_list], '.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not suffix_setting == '_rj_custom':\n",
    "    cluster_assignments_dict = joblib.load(join(\n",
    "        cache_ngrams_dir, f'cluster_assignments_dict_{subject}{suffix_setting}.jbl'))\n",
    "cluster_sizes_list = [\n",
    "    np.unique(\n",
    "        cluster_assignments_dict[k][cluster_assignments_dict[k] >= 0], return_counts=True)[1]\n",
    "    for k in cluster_assignments_dict\n",
    "]\n",
    "df_sizes = pd.DataFrame([list(x) for x in cluster_sizes_list], index=k_values)\n",
    "df_sizes = df_sizes.stack().reset_index().set_index('level_0')\n",
    "\n",
    "df = pd.DataFrame([list(x) for x in cluster_corrs_list], index=k_values)\n",
    "df = df.stack().reset_index().set_index('level_0')\n",
    "df['size'] = df_sizes[0].values\n",
    "df['size>100'] = df['size'] > 100\n",
    "df['size_str'] = df['size'].apply(\n",
    "    lambda x: '<10' if x < 10 else '10-50' if x < 50 else '50-100' if x < 100 else '>100')\n",
    "df.index.names = ['num_clusters']\n",
    "df = df.rename(columns={'level_1': 'cluster_idx', 0: 'corr'}).reset_index()\n",
    "\n",
    "# select some clusters\n",
    "# num_clusters_selected = 123\n",
    "# num_clusters_selected = 287\n",
    "# num_clusters_selected = 316\n",
    "\n",
    "if suffix_setting == '_rj_custom':\n",
    "    d = df\n",
    "    num_clusters_selected = 9\n",
    "else:\n",
    "    num_clusters_selected = 1000\n",
    "    d = df[df['num_clusters'] == num_clusters_selected]\n",
    "\n",
    "# d = d[d['size'] >= 10]\n",
    "d_selected = d.sort_values('corr', ascending=False).head(200)\n",
    "\n",
    "# add indexes voxels\n",
    "idxs_vox_list = []\n",
    "for i, row in tqdm(d_selected.iterrows()):\n",
    "    cluster_idx = row['cluster_idx']\n",
    "    flatmap = np.zeros(SUBJECT_VOX_COUNTS_DICT[subject])\n",
    "    # idxs_pfc_filt = np.where(\n",
    "    # cluster_assignments_dict[num_clusters_selected] == cluster_idx)[0]\n",
    "    idxs_pfc_filt = cluster_assignments_dict[num_clusters_selected] == cluster_idx\n",
    "    idxs = np.zeros(SUBJECT_VOX_COUNTS_DICT[subject])\n",
    "    pfc_filt = pfc.copy()\n",
    "    pfc_filt[mod.corrs < 0.15] = False\n",
    "    idxs[pfc_filt] = idxs_pfc_filt\n",
    "    idxs_vox_list.append(np.where(idxs)[0])\n",
    "d_selected['idxs_vox'] = idxs_vox_list\n",
    "d_selected['corr_test'] = d_selected['idxs_vox'].apply(\n",
    "    lambda x: mod.corrs[x].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make wide figure\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# color with discrete colormap\n",
    "sns.pointplot(data=df, x='num_clusters', y='corr', errorbar=None,\n",
    "              color='black', markers='.', alpha=0.5)\n",
    "ax = sns.stripplot(data=df, x='num_clusters', y='corr', jitter=0.2,\n",
    "                   alpha=1, size=3, hue='size_str')\n",
    "ax.get_legend().set_title('Voxel count in the cluster')\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Stability score (per cluster)')\n",
    "# plt.axhline(0, color='gray', linewidth=0.5)\n",
    "plt.grid()\n",
    "\n",
    "# rotate xlabels\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in tqdm(d_selected.iterrows()):\n",
    "    flatmap = np.zeros(SUBJECT_VOX_COUNTS_DICT[subject])\n",
    "    flatmap[pfc][row['idxs_vox']] = 1\n",
    "    sasc.viz._save_flatmap(\n",
    "        flatmap, 'S02', fname_save=f'clusters_selected/{i+1}.png')\n",
    "\n",
    "\n",
    "# read all plots and save as subplots on the same page\n",
    "C = 3\n",
    "R = ceil(len(d_selected)/C)\n",
    "fig, axs = plt.subplots(R, C, figsize=(C * 4, R * 2))\n",
    "axs = axs.ravel()\n",
    "j = 0\n",
    "for i, row in d_selected.iterrows():\n",
    "    axs[j].imshow(Image.open(f'clusters_selected/{i+1}.png'))\n",
    "    axs[j].axis('off')\n",
    "    axs[j].set_title(f'stab {row[\"corr\"]:.2f}, {row[\"size\"]:.0f} voxels')\n",
    "    j += 1\n",
    "plt.savefig(f'clusters_selected{suffix_setting}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dict = joblib.load(\n",
    "    join(cache_ngrams_dir, f'pfc_clusters_ngram_outputs_dict_{num_clusters_selected}_{subject}_opt{suffix_setting}.pkl'))\n",
    "df_opt = pd.DataFrame(outputs_dict, index=ngrams_list)\n",
    "outputs_dict = joblib.load(\n",
    "    join(cache_ngrams_dir, f'pfc_clusters_ngram_outputs_dict_{num_clusters_selected}_{subject}_llama{suffix_setting}.pkl'))\n",
    "df_llama = pd.DataFrame(outputs_dict, index=ngrams_list)\n",
    "df = df_opt + df_llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascending = False  # should be false to get driving ngrams\n",
    "# top_ngrams_dict = {}\n",
    "# for k in df.columns:\n",
    "# top_ngrams_dict[k] = df.sort_values(\n",
    "# k, ascending=ascending).index[:100].tolist()\n",
    "# if k in ROIS_LOC:\n",
    "\n",
    "# top_ngrams_dict[k + '_only'] = df.sort_values(\n",
    "# k + '_only', ascending=ascending).index[:100].tolist()\n",
    "# top_ngrams_df = pd.DataFrame(top_ngrams_dict)\n",
    "d_selected['top_ngrams'] = d_selected['cluster_idx'].apply(\n",
    "    lambda x: df.loc[:, x].sort_values(ascending=False).index[:100].tolist())\n",
    "# top_ngrams_df.to_csv(f'top_ngrams_by_pfc_cluster_{subject}.csv')\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "# rois = ['RSC', 'OPA', 'PPA', 'IPS', 'pSTS', 'sPMv',\n",
    "# 'EBA', 'OFA'] + ['RSC_only', 'OPA_only', 'PPA_only']\n",
    "# rois = [r for r in rois if not r == 'pSTS']  # never localized pSTS in S03\n",
    "# display(top_ngrams_df[rois])\n",
    "d_selected.to_pickle(\n",
    "    f'top_clusters_by_pfc_cluster_{subject}{suffix_setting}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate explanations\n",
    "# d_selected = pd.read_pickle(\n",
    "#     f'top_clusters_by_pfc_cluster_{subject}{suffix_setting}.pkl')\n",
    "# gpt4 = imodelsx.llm.get_llm('gpt-4')\n",
    "# explanations = []\n",
    "# prompt_template = '''Here is a list of phrases:\n",
    "# {s}\n",
    "\n",
    "# What is a common theme among these phrases? Return only a concise phrase.'''\n",
    "# for i, row in d_selected.iterrows():\n",
    "#     top_ngrams = row['top_ngrams']\n",
    "#     s = '- ' + '\\n- '.join(top_ngrams[:60])\n",
    "#     prompt = prompt_template.format(s=s)\n",
    "#     # print(prompt)\n",
    "#     # explanations.append(gpt4(prompt))\n",
    "# # joblib.dump(explanations, f'explanations_by_pfc_cluster_{subject}.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLANATIONS_ALREADY_RAN_S02_QA = [\n",
    "    'abstract descriptions',\n",
    "    'numbers',\n",
    "    'specific objects or items',\n",
    "    'industry or profession',\n",
    "    'sensory experiences',\n",
    "    'negations',\n",
    "    'time',\n",
    "    'personal reflections or thoughts',\n",
    "    'opinions or judgments',\n",
    "    'technical or specialized terminology',\n",
    "    'personal values or beliefs',\n",
    "    'planning or organizing',\n",
    "    'proper nouns',\n",
    "    'physical actions',\n",
    "    'personal or interactions interactions',\n",
    "    'relationships between people',\n",
    "    'cultural references',\n",
    "    'communication',\n",
    "    'dialogue',\n",
    "    'locations',\n",
    "    'measurements',\n",
    "]\n",
    "\n",
    "EXPLANATIONS_ALREADY_RAN_S02_VOXEL = [\n",
    "    'surprise',\n",
    "    'laughter',\n",
    "    'moments',\n",
    "    'rejection',\n",
    "    'emotion',\n",
    "    'negativity',\n",
    "    'emotional expression',\n",
    "    'communication',\n",
    "    'death',\n",
    "    'birthdays',\n",
    "    'physical injury or trauma',\n",
    "    'hair and clothing',\n",
    "    'food preparation',\n",
    "    'time',\n",
    "    'measurements',\n",
    "    'directions',\n",
    "    'locations',\n",
    "]\n",
    "\n",
    "EXPLANATIONS_ALREADY_RAN_S02_ROIS = [\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = joblib.load(\n",
    "    f'explanations_by_pfc_cluster_{subject}{suffix_setting}.jbl')\n",
    "# d_selected['explanation'] = explanations\n",
    "d_selected.insert(0, 'explanation', explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_out = (d_selected\n",
    "         .rename(columns={'corr': 'stability score'})\n",
    "         .drop(\n",
    "             columns=['cluster_idx', 'num_clusters', 'size>100', 'size_str'])\n",
    "         .round(3)\n",
    "         .infer_objects()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_out.to_csv(f'top_clusters_by_pfc_cluster_{subject}{suffix_setting}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S03 Export selected rois to pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = ['RSC', 'OPA', 'PPA', 'IPS', 'sPMv', 'EBA', 'OFA'] + \\\n",
    "    ['RSC_only', 'OPA_only', 'PPA_only']  # all but 'pSTS'\n",
    "# pprint({k: explanations[k] for k in rois})\n",
    "explanations_clean = {\n",
    "    'EBA': 'Body parts',\n",
    "    'IPS': 'Descriptive elements of scenes or objects',\n",
    "    # OFA differs from UTS02 (which was \"'Personal growth and reflection',\")\n",
    "    'OFA': 'Conversational transitions',\n",
    "    'OPA': 'Direction and location descriptions',\n",
    "    # OPA_only differs from UTS02 (which was 'Spatial positioning and directions')\n",
    "    'OPA_only': 'Self-reflection and growth',\n",
    "    'PPA': 'Scenes and settings',\n",
    "    'PPA_only': 'Unappetizing foods',\n",
    "    'RSC': 'Travel and location names',\n",
    "    'RSC_only': 'Location names',\n",
    "    # sPMv differs from UTS02 (which was 'Time and Numbers')\n",
    "    'sPMv': 'Dialogue and responses',\n",
    "}\n",
    "explanation_avoid_suffixes = {\n",
    "    'EBA': ' Avoid mentioning any locations.',\n",
    "    'IPS': ' Avoid mentioning any locations.',\n",
    "    'OFA': ' Avoid mentioning any locations.',\n",
    "    'OPA': ' Avoid mentioning any specific location names (like \"New York\" or \"Europe\").',\n",
    "    'OPA_only': ' Avoid mentioning any specific location names (like \"New York\" or \"Europe\").',\n",
    "    'PPA': ' Avoid mentioning any specific location names (like \"New York\" or \"Europe\").',\n",
    "    'PPA_only': ' Avoid mentioning any specific location names (like \"New York\" or \"Europe\").',\n",
    "    'RSC': '',\n",
    "    'RSC_only': '',\n",
    "    'sPMv': ' Avoid mentioning any locations.'\n",
    "}\n",
    "for roi in rois:\n",
    "    print(f'\"{roi}\":', str(\n",
    "        top_ngrams_df[roi.replace('1', '').replace('2', '')].iloc[:50].values.tolist()) + ', ')\n",
    "top_ngrams_clean = {\n",
    "    \"RSC\": ['was led upstairs', 'onto the subway', 'to the hallway', 'drove to washington', 'back through london', 'and darted downstairs', 'past the offices', 'long hallway toward', 'down the sidewalk', 'back in manhattan', 'reached the interstate', 'just blocks away', 'drove from vermont', 'was standing outside', 'to a courtyard', 'in the alley', 'up the coast', 'from my dorm', 'in the courtyard', 'in central park', 'i walk outside', 'here in manhattan', 'darted downstairs', 'facing the beach', 'walk through downtown', 'wander the hallways', 'i ran downstairs', 'down the hall', \"'m standing outside\", 'off into vancouver', 'through the streets', 'sitting in indianapolis', 'on sixth avenue', 'i go upstairs', 'across the street', 'arrived in indianapolis', 'we were downtown'],\n",
    "    \"OPA\": ['railing looking out', 'across a plateau', 'up the coast', 'against the railing', 'in the courtyard', 'up the hill', 'above the gulf', 'outside the windows', 'long hallway toward', 'over the gulf', 'past the offices', 'through the windows', 'beside the river', 'past the waterfall', 'across the bridge', 'this long hallway', 'to a courtyard', 'and the courtyard', 'and behind me', 'down this embankment', 'towards the river', 'the hill up', 'courtyard was surrounded', 'in an alcove', 'onto the railing', 'along the coast', 'up the stairs', 'across the quadrangle', 'facing the beach', 'to the north', 'down the corridor', 'through the gates', 'over the embankment', 'onto the bridge', 'down that corridor', 'down the sidewalk', 'i looked across', 'path that jutted', 'through this door', 'the lagoon behind', 'down the embankment', 'on the railing', 'on the embankment', 'through the doors', 'on the windowsill', 'corridor out onto', 'the buildings beside', 'to the hallway', 'by that window', 'past the city', 'door behind me', 'to the south', 'off the coast', 'cross the bering', 'around the reef', 'behind me i', 'driveway and behind', 'against the windows', 'across the street', 'to the shoreline', 'lagoon behind the', 'on the sidewalk', 'hall past the', 'off the east', 'of the ravine', 'surrounded the city', 'in the window', 'southern shore of', 'in the distance', 'onto the sidewalk', 'i look across', 'behind us i', 'behind us there', 'on the cliff', 'over the river', 'toward the ocean', 'on that terrace', 'row of stalls', 'sidewalk in front', 'down the long', 'on the walls', 'door to the', 'by the window', 'outside my door', 'outside the door', 'across from me', 'on the eastern', 'the hall past', 'down the lagoon', 'in the forest', 'that window in', 'around me the', 'to the barrier', 'the gulf where', 'road in front', 'in the hallway', 'across the parking', 'in the colonnade', 'to the western', 'surrounded by rooms'],\n",
    "    \"PPA\": ['in an alcove', 'on the stoop', 'past the offices', 'against the railing', 'on the windowsill', 'in the alley', 'to a courtyard', 'the copier room', 'in the courtyard', 'this long hallway', 'to the hallway', 'on a dock', 'in the hallway', 'long hallway toward', 'outside the windows', 'on that terrace', 'inside the hut', 'railing looking out', 'through the windows', 'down this embankment', 'on the subway', 'onto the subway', 'there were shelves', 'in my cubicle', 'a strip mall', 'on the sidewalk', 'in the colonnade', 'on the railing', 'into the basement', 'across the parking', 'a restaurant stoop', 'onto the railing', 'exit the subway', 'by the window', 'in that attic', 'was led upstairs', 'in the basement', 'the food court', 'and the courtyard', 'in the cafeteria', 'hall past the', 'into the parking', 'in the windowless', 'back room where', 'on my bed', 'down the sidewalk', 'contain strip malls', 'onto the sidewalk', 'the hall closet', 'at those cliffs'],\n",
    "\n",
    "    \"RSC_only\": ['moved to chicago', 'drove from vermont', 'came to florida', 'here in manhattan', 'living in chicago',  'move to texas', 'leaving for france', 'back in manhattan', 'to boston to', 'went to boston', 'moved to vermont', 'geese in ohio', 'college in boston', 'in ohio', 'moved to brooklyn', 'normal suburban pittsburgh', 'moved to london', 'back in israel', 'to london to', 'come from israel', 'went to manchester', 'to columbus ohio', 'here in boston', 'i left vermont', 'from pittsburgh pennsylvania', 'in lower manhattan', 'hometown in texas', 'touring through europe', 'in warmer mexico', 'union in manhattan', 'suburban pittsburgh', 'moved to washington', 'was in boston', 'slacking in madison', 'chick from silverlake', 'heading to iraq', 'in chicago', 'in louisville kentucky', 'lived in hiroshima', 'in florida'],\n",
    "    \"OPA_only\": ['eventually i forgave', 'push past it', 'eventually forgave', 'she eventually forgave', 'i forgave', 'to see ourselves', 'of myself which', 'means extending empathy', 'forgive and', 'i stopped myself', 'forgive and love', 'of the hurt', 'i rise above', 'i pushed myself', 'is and who', \"'m hurt but\", 'see ourselves and', 'the hurt', 'i persisted and', 'to forgive afterwards', 'was real to', 'comparing myself', 'looked in myself', 'selves which', 'around it and', 'self which translated', 'inside me that', 'of me which', 'overcome my ambivalence', 'to push myself', 'was also influenced', 'the side which', 'side which', 'independence and freedom'],\n",
    "    \"PPA_only\": ['a garbage bag', 'that garbage bag', \"'re throwing napkins\", 'box of discarded', 'in sugar jars', 'their chew toys', 'those plastic containers', 'our dishwasher', 'skivvies toothbrush floss', 'of cheap beer', 'throwing napkins', 'a trash can', 'milk bottle tops', 'want a mcflurry', 'vomit smelling couch', 'grown napkins', 'my cheese sandwich', 'overpriced coffee shops', 'some lighter fluid', 'salad and stale', 'vomited a washpan', 'these brown paper'],\n",
    "\n",
    "    \"IPS\": ['and behind me', 'against the railing', 'onto the railing', 'path that jutted', 'situated herself behind', 'above the gulf', 'door behind me', 'southern shore of', 'i looked across', 'along the edge', 'closed behind me', 'behind me and', 'across a plateau', 'on the railing', 'up behind me', 'leaning against the', 'towering above me', 'jutted into the', 'onto the bridge', 'and cut across', 'behind him and', 'and came around', 'front of us', 'up onto the', 'over the gulf', 'stood behind me', 'across the bridge', 'beside the river'],\n",
    "    \"sPMv\": ['repeated her affirmation', 'said excuse me', 'asked i laughed', 'and goes hey', 'response was nah', 'hurry she exclaimed', 'said no i', 'just nodded yes', 'retorted rather loudly', 'was like hey', 'called her and', 'and said yes', 'and screams fuck', 'said uh hey', 'says sure and', 'says uh actually', 'was like hi', 'i said fine', 'said without hesitation', 'said well yes', 'says excuse me', 'i asked immediately', 'she yelled i', 'said mom mom', 'said did i', 'i said wow', 'i said shyly', 'asked her and', 'said okay okay', 'i sheepishly raise', 'which i responded', 'turned and said', 'then wrote yes', 'said yes i', 'whisper she said', 'was like mhm'],\n",
    "    \"EBA\": ['arms around her', 'wraps his arms', 'hands gripped the', 'into my palm', 'hands into my', 'elbows on knees', 'grab his arms', 'his hands folded', 'into her arms', 'grabbed her hand', 'arms flailing', 'grabbed her legs', 'arm around my', 'grabbed their hands', 'lifted her up', 'put my arms', 'leaned his head', 'put his arms', 'shakes my hand', 'flying arms flailing', 'i leaned down', 'arms tighten around', 'her hands gripped', 'hand on his', 'my feet kicking', 'pinning my arms', 'held her hand', 'in a headlock', 'pressed my face', 'holds her hand', 'arms flailing holding', 'rubbing his head'],\n",
    "    \"OFA\": ['and we talked', 'i even met', 'and so finally', 'one night my', 'one evening after', 'anyway the point', 'one summer my', 'weeks passed and', 'finally one day', 'then we talked', 'one night i', 'we chatted', 'and i talked', 'we talked and', 'talked and', 'so i texted', 'to my surprise', 'one afternoon when', 'i persisted and', 'and i finally', 'was watching television', 'i remember once', 'so one night', 'but anyway', 'and i met', 'when i finally', 'so i finally', 'on and on', 'son had finally', 'and i especially', 'so one day', 'and eventually i', 'we brace ourselves', 'so anyway', 'i was perusing', 'and it finally', 'later that day', 'so we finally', 'but anyhow', 'and as we', 'the day came', 'home one afternoon', 'then i finally', 'what fascinated me', 'and i vaguely', 'so i talked', 'once while i', 'was hanging out', 'i was reliving', 'but the most'],\n",
    "}\n",
    "\n",
    "rows = {\n",
    "    'roi': rois,\n",
    "    'expl': [explanations_clean[k] for k in rois],\n",
    "    'top_ngrams_module_correct': [top_ngrams_clean[k] for k in rois],\n",
    "    'stability_score': [stability_scores[k.split('_')[0]] for k in rois],\n",
    "    'subject': [f'UT{subject}'] * len(rois),\n",
    "    'voxel_nums': [rois_dict[k.split('_')[0]] for k in rois],\n",
    "    'prompt_suffix': [explanation_avoid_suffixes[k] for k in rois],\n",
    "}\n",
    "rows = pd.DataFrame(rows)\n",
    "rows.to_pickle(f'rows_roi_ut{subject.lower()}_may31.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
