{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import joblib\n",
    "from pprint import pprint\n",
    "from sklearn.preprocessing import normalize\n",
    "from math import ceil\n",
    "import cortex\n",
    "from neuro import config\n",
    "from collections import defaultdict\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from neuro import flatmaps_helper\n",
    "from neuro.flatmaps_helper import load_flatmaps\n",
    "import neuro.sasc.viz\n",
    "import neuro.viz\n",
    "from neuro import analyze_helper\n",
    "import nibabel as nib\n",
    "neurosynth_compare = __import__('04_neurosynth_compare')\n",
    "import neurosynth\n",
    "from neuro.features.questions.gpt4 import QS_35_STABLE\n",
    "from neuro.features import qa_questions\n",
    "import dvu\n",
    "dvu.set_style()\n",
    "\n",
    "config.setup_freesurfer()\n",
    "N_SURVEY_RESPONSES = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensemble (non-gpt-4) feats each run one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_all = pd.read_pickle(join(config.RESULTS_DIR_LOCAL, 'results_full_oct17.pkl'))\n",
    "r = rr_all[rr_all.ndelays == 4]\n",
    "r = r[r.pc_components == 100]\n",
    "r = r[r.feature_space == 'qa_embedder']\n",
    "r = r[r.qa_questions_version == 'v3_boostexamples_merged']\n",
    "r = r[r.qa_embedding_model == 'ensemble2']\n",
    "r = r[r.single_question_idx >= 0]\n",
    "r = r[r.feature_selection_alpha == -1]\n",
    "\n",
    "ravg = r.groupby(['single_question_idx'])[\n",
    "    ['corrs_test_mean']].mean().reset_index()\n",
    "qs = qa_questions.get_merged_questions_v3_boostexamples()\n",
    "ravg['question'] = ravg['single_question_idx'].apply(lambda i: qs[i])\n",
    "ravg['q_selected'] = ravg['question'].apply(lambda q: q in QS_35_STABLE)\n",
    "ravg['question_abbrev'] = ravg['question'].apply(\n",
    "    analyze_helper.abbrev_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize top and bottom questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ravg.to_pickle('corrs_df/single_question_corrs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means 0.024616516749857795 0.01474965305918737\n"
     ]
    }
   ],
   "source": [
    "r_selected = ravg[ravg.q_selected]\n",
    "r_unselected = ravg[~ravg.q_selected]\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.histplot(r_unselected.corrs_test_mean, label='Unselected', color=\"C1\")\n",
    "sns.histplot(r_selected.corrs_test_mean, label='Selected 35', color=\"C0\")\n",
    "plt.legend()\n",
    "plt.xlabel('Test correlation using single-question model')\n",
    "plt.ylabel('Question count')\n",
    "print('means', r_selected.corrs_test_mean.mean(),\n",
    "      r_unselected.corrs_test_mean.mean())\n",
    "neuro.viz.savefig(\n",
    "    'monosemantic/single_question_perf_hists.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "question_abbrev & corrs_test_mean \\\\\n",
      "\\midrule\n",
      "...contain words with strong visual imagery? & 0.036 \\\\\n",
      "...part of a narrative? & 0.033 \\\\\n",
      "...describe an act of communication or interaction with others? & 0.030 \\\\\n",
      "...describe a an interpersonal misunderstanding or dispute? & 0.030 \\\\\n",
      "...describe an emotional response to a specific event? & 0.030 \\\\\n",
      "...mention of a physical object or item? & 0.030 \\\\\n",
      "...involve a social or interpersonal interaction? & 0.030 \\\\\n",
      "...involve the description of an emotional response? & 0.030 \\\\\n",
      "...mention of a scientific fact or concept? & 0.030 \\\\\n",
      "...describe a an emotional reaction to a surprise or unexpected event? & 0.030 \\\\\n",
      "...use irony or sarcasm? & 0.029 \\\\\n",
      "...contain a first-person narrative? & 0.029 \\\\\n",
      "...contain a description of an interaction that led to a misunderstanding or conflict? & 0.029 \\\\\n",
      "...first-person pronoun in the input? & 0.029 \\\\\n",
      "...include a recounting of an impactful or emotional dialogue? & 0.029 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "question_abbrev & corrs_test_mean \\\\\n",
      "\\midrule\n",
      "...part of a legal document or text? & 0.002 \\\\\n",
      "...mention a fitness or sports achievement? & 0.002 \\\\\n",
      "...religious or spiritual practice mentioned? & 0.002 \\\\\n",
      "...describe a scientific experiment or discovery? & 0.002 \\\\\n",
      "...referencing legal matters or rights? & 0.002 \\\\\n",
      "...involve a financial planning or investment strategy? & 0.002 \\\\\n",
      "...discuss a breakthrough in medical research? & 0.001 \\\\\n",
      "...mention of a war, battle, or military event? & 0.001 \\\\\n",
      "...a well-known quote or saying? & 0.000 \\\\\n",
      "...involve a coding or programming concept? & 0.000 \\\\\n",
      "...discussion about politics or government? & -0.000 \\\\\n",
      "...discuss a natural disaster or emergency situation? & -0.001 \\\\\n",
      "...educational lesson or class described? & -0.001 \\\\\n",
      "...reference a religious or spiritual concept? & -0.002 \\\\\n",
      "...mention of a natural disaster? & -0.002 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with pd.option_context('display.max_colwidth', None,\n",
    "#                        'display.max_rows', None):\n",
    "#     display(ravg[~ravg.q_selected].sort_values('corrs_test_mean', ascending=False)\n",
    "#             [['question_abbrev', 'corrs_test_mean']].head(20))\n",
    "print(\n",
    "    ravg[~ravg.q_selected]\n",
    "    .sort_values('corrs_test_mean', ascending=False)[['question_abbrev', 'corrs_test_mean']]\n",
    "    .head(15).to_latex(float_format=\"%.3f\", index=False)\n",
    ")\n",
    "print(\n",
    "    ravg[~ravg.q_selected]\n",
    "    .sort_values('corrs_test_mean', ascending=False)[['question_abbrev', 'corrs_test_mean']]\n",
    "    .tail(15).to_latex(float_format=\"%.3f\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load survey results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_results = pd.read_csv('survey_results.csv')\n",
    "# set first column name to 'question_abbrev'\n",
    "survey_results.rename(columns={survey_results.columns[0]: 'question_abbrev'}, inplace=True)\n",
    "\n",
    "# apply lambda function to col names\n",
    "survey_results.columns = [x[:x.index('(')].strip() if '(' in x else x for x in survey_results.columns]\n",
    "# add question mark to question_abbrev\n",
    "\n",
    "def remove_parens(s):\n",
    "    if '(' in s and ')' in s:\n",
    "        # remove everything from the first '(' to the first ')'\n",
    "        # and return the rest of the string\n",
    "        s = s[:s.index('(')] + s[s.index(')') + 1:]\n",
    "    return s\n",
    "\n",
    "survey_results['question_abbrev'] = survey_results['question_abbrev'].apply(\n",
    "    remove_parens)\n",
    "survey_results['question_abbrev'] = survey_results['question_abbrev'].apply(\n",
    "    lambda x: x.strip() + '?' if not x.strip().endswith('?') else x)\n",
    "\n",
    "RENAME_DICT = {\n",
    "    '...contain numbers?': '...contain a number?',\n",
    "    \"...are reflective, involving self-analysis or introspection?\":\"...reflective, involving self-analysis or introspection?\",\n",
    "    \"...are related to a specific industry or profession?\":\n",
    "        \"...related to a specific industry or profession?\",\n",
    "     \"...is abstract rather than concrete?\":\n",
    "        \"...abstract rather than concrete?\",\n",
    "    \"...describe an interpersonal misunderstanding or dispute?\":\n",
    "        \"...describe a an interpersonal misunderstanding or dispute?\",\n",
    "     \"...cointain first-person pronoun?\":\n",
    "        \"...first-person pronoun in the input?\",\n",
    "    \"...are part of a legal document or text?\":\n",
    "        \"...part of a legal document or text?\",\n",
    "    '...include a description about dialogue?':    \n",
    "        '...include dialogue?',\n",
    "    '...describe an educational lesson or class?':\n",
    "        '...educational lesson or class described?',\n",
    "}\n",
    "# rename columns according to RENAME_DICT\n",
    "survey_results['question_abbrev'] = [RENAME_DICT.get(x, x) for x in survey_results['question_abbrev']]\n",
    "\n",
    "\n",
    "# merge with ravg on question_abbrev\n",
    "merged = ravg.merge(survey_results, on='question_abbrev', how='right')\n",
    "\n",
    "# for any duplicate question_abbrev, take the one where q_selected is True\n",
    "merged = merged.sort_values('q_selected', ascending=False).drop_duplicates(\n",
    "    'question_abbrev').sort_index()\n",
    "\n",
    "merged['Standard Error'] = merged['Standard Deviation'] / np.sqrt(N_SURVEY_RESPONSES)\n",
    "merged['legend'] = merged['q_selected'].apply(\n",
    "    lambda x: 'Selected 35' if x else 'Unselected')\n",
    "merged.sort_values('corrs_test_mean', ascending=False, inplace=True)\n",
    "\n",
    "# add categories\n",
    "categories = pd.read_csv('survey_results_merged.csv')\n",
    "merged = pd.merge(merged, categories, on='question', how='inner')\n",
    "\n",
    "# rewrite categories\n",
    "merged['category'] = merged['category'].apply(\n",
    "    lambda x: x.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merged = merged[['question_abbrev', 'Average', 'Standard Deviation', 'corrs_test_mean']].sort_values(by='corrs_test_mean', ascending=False)\n",
    "# with pd.option_context('display.max_colwidth', None,\n",
    "#                        'display.max_rows', None):\n",
    "#     display(merged[merged.corrs_test_mean.isna()]['question_abbrev'])\n",
    "#     # display(ravg.question_abbrev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.errorbar(merged['corrs_test_mean'], merged['Average'],\n",
    "             yerr=merged['Standard Error'], fmt='none', capsize=2,\n",
    "             elinewidth=1, alpha=0.35, color='gray', zorder=-11)\n",
    "sns.scatterplot(data=merged, x='corrs_test_mean', y='Average',\n",
    "                hue='category',\n",
    "                hue_order=[\n",
    "                    'Visuospatial information', 'Communication',\n",
    "                'Abstract beliefs or values', 'Numerical information',\n",
    "                'Tactile sensations', 'Other', ],\n",
    "                s=100, alpha=0.9)\n",
    "plt.xlabel('Test correlation using single-question model')\n",
    "plt.ylabel('Expert rating')\n",
    "plt.legend(title='', frameon=False, handletextpad=0.1)\n",
    "neuro.viz.savefig(\n",
    "    'monosemantic/single_question_perf_vs_survey.png', bbox_inches='tight', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Visuospatial information', 'Communication',\n",
       "       'Abstract beliefs or values', 'Other', 'Numerical information',\n",
       "       'Tactile sensations'], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Below is a list of questions. For each question, list which of these 7 categories it belongs in:\n",
    "\n",
    "Categories:\n",
    "- tactile sensations\n",
    "- visuospatial information\n",
    "- numerical information\n",
    "- planning\n",
    "- communication\n",
    "- abstract beliefs or values\n",
    "- other\n",
    "\n",
    "Questions:\n",
    "[\n",
    "    'Does the sentence contain words with strong visual imagery?',\n",
    "    'Does the sentence describe a personal or social interaction that leads to a change or revelation?',\n",
    "    'Does the sentence involve a description of physical environment or setting?',\n",
    "    'Does the sentence describe a relationship between people?',\n",
    "    'Does the sentence involve a description of an interpersonal misunderstanding or dispute?',\n",
    "    'Does the sentence mention a specific location?',\n",
    "    'Does the sentence involve the description of an emotional response?',\n",
    "    'Does the sentence use irony or sarcasm?',\n",
    "    'Does the sentence involve the mention of a specific object or item?',\n",
    "    'Is there a first-person pronoun in the input?',\n",
    "    'Does the sentence include technical or specialized terminology?',\n",
    "    'Does the sentence involve spatial reasoning?',\n",
    "    'Does the sentence involve a discussion about personal or social values?',\n",
    "    'Does the sentence include a personal anecdote or story?',\n",
    "    'Does the sentence describe a visual experience or scene?',\n",
    "    'Is time mentioned in the input?',\n",
    "    'Does the text describe a journey?',\n",
    "    \"Does the sentence express the narrator's opinion or judgment about an event or character?\",\n",
    "    'Does the sentence involve an expression of personal values or beliefs?',\n",
    "    'Does the sentence describe a physical action?',\n",
    "    'Does the input contain a measurement?',\n",
    "    'Does the text describe a mode of communication?',\n",
    "    'Does the sentence describe a personal reflection or thought?',\n",
    "    'Does the sentence contain a proper noun?',\n",
    "    'Does the sentence include a direct speech quotation?',\n",
    "    'Does the input contain a number?',\n",
    "    'Does the sentence contain a cultural reference?',\n",
    "    'Does the sentence contain a negation?',\n",
    "    'Is the sentence reflective, involving self-analysis or introspection?',\n",
    "    'Does the sentence express a sense of belonging or connection to a place or community?',\n",
    "    'Is the sentence abstract rather than concrete?',\n",
    "    'Does the sentence describe a specific sensation or feeling?',\n",
    "    'Does the sentence include dialogue?',\n",
    "    'Does the input include a comparison or metaphor?',\n",
    "    'Does the input involve planning or organizing?',\n",
    "    'Does the input describe a specific texture or sensation?',\n",
    "    'Does the sentence describe a sensory experience?',\n",
    "    'Does the sentence describe a physical sensation?',\n",
    "    'Does the text include a planning or decision-making process?',\n",
    "    'Is the input related to a specific industry or profession?',\n",
    "    'Is the sentence part of a legal document or text?',\n",
    "    'Does the input describe a scientific experiment or discovery?',\n",
    "    'Does the input discuss a breakthrough in medical research?',\n",
    "    'Does the input involve a coding or programming concept?',\n",
    "    'Is an educational lesson or class described?'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The rest of this is experimental, never ran it properly..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-4 feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = pd.read_pickle(join(config.RESULTS_DIR_LOCAL, 'oct17_tmp.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'S02'\n",
    "r = rr\n",
    "r = r[r.subject == subject]\n",
    "r = r[r.use_added_wordrate_feature == False]\n",
    "r = r[r.feature_space == 'qa_embedder']\n",
    "r = r[r.qa_embedding_model == 'gpt4']\n",
    "r = r[r.qa_questions_version.str.endswith('?')]  # individual question\n",
    "r = r[r.ndelays == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = pd.read_pickle('../notebooks/monosemantic_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_dicts = {}\n",
    "for subj in r.subject.unique():\n",
    "    r_subj = r[r.subject == subj]\n",
    "    q_to_corrs = r_subj.set_index(\n",
    "        'qa_questions_version').corrs_test.to_dict()\n",
    "\n",
    "    vox_to_q = df_selected[df_selected.subject == 'UT' + subj]\n",
    "    vox_to_q_dict = vox_to_q.set_index('voxel_idx').question.to_dict()\n",
    "\n",
    "    corrs = np.zeros(len(vox_to_q_dict))\n",
    "    for i, (vox, question) in enumerate(tqdm(vox_to_q_dict.items())):\n",
    "        corrs[i] = q_to_corrs[question][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_to_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vox_to_question_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r_subj.set_index('qa_questions_version')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
