{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "import joblib\n",
    "import sasc.config\n",
    "import numpy as np\n",
    "import re\n",
    "import sasc.viz\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from sasc import config\n",
    "import pandas as pd\n",
    "from sasc import analyze_helper\n",
    "\n",
    "pilot_name = 'pilot_story_data.pkl'\n",
    "# pilot_name = 'pilot3_story_data.pkl'\n",
    "# pilot_name = 'pilot4_story_data.pkl'\n",
    "\n",
    "stories_data_dict = joblib.load(\n",
    "    join(config.RESULTS_DIR, 'processed', pilot_name))\n",
    "if pilot_name == 'pilot_story_data.pkl':\n",
    "    pilot_data_dir = join(config.PILOT_STORY_DATA_DIR, '20230504')\n",
    "elif pilot_name == 'pilot3_story_data.pkl':\n",
    "    pilot_data_dir = join(config.PILOT_STORY_DATA_DIR, '20231106')\n",
    "elif pilot_name == 'pilot4_story_data.pkl':\n",
    "    pilot_data_dir = join(config.PILOT_STORY_DATA_DIR, '20240509')\n",
    "\n",
    "# pilot_data_dir = join(sasc.config.FMRI_DIR, 'story_data/20230504')\n",
    "resp_np_files = os.listdir(pilot_data_dir)\n",
    "resps_dict = {k: np.load(join(pilot_data_dir, k)) for k in tqdm(resp_np_files)}\n",
    "stories_data_dict = joblib.load(\n",
    "    join(config.RESULTS_DIR, 'processed', pilot_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these function use absolute timing indexes rather than trimmed ones. This works out because they never have to reference the paragraph splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot annotated response curves and aggregate word_chunk_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:12<00:00,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta 1 mean: 0.1107 +/- 0.0594\n",
      "Delta 2 mean: 0.1489 +/- 0.0697\n",
      "Delta 3 mean: 0.1794 +/- 0.0747\n",
      "Delta 4 mean: 0.1315 +/- 0.0828\n",
      "Delta 5 mean: 0.0528 +/- 0.0836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_chunk_deltas = []\n",
    "for story_num in tqdm(range(6)):\n",
    "    paragraphs = stories_data_dict[\"story_text\"][story_num].split(\"\\n\\n\")\n",
    "    prompts = stories_data_dict[\"prompts\"]\n",
    "    t = deepcopy(stories_data_dict[\"timing\"][story_num])\n",
    "    # t['time_running'] = t['time_running'] - 10\n",
    "\n",
    "    # these must be passed same timing\n",
    "    word_chunks = analyze_helper._get_word_chunks(t)\n",
    "    # word_chunks = word_chunks[5:-5]\n",
    "    start_times, end_times = analyze_helper.get_start_end_indexes_for_paragraphs(\n",
    "        t, paragraphs)\n",
    "\n",
    "    # example_ngrams and word_chunks\n",
    "    ps = prompts[story_num]\n",
    "    example_ngrams_list = sum(\n",
    "        [analyze_helper.find_all_examples_within_quotes(x) for x in ps], []\n",
    "    )\n",
    "    word_chunks_contain_example_ngrams = np.zeros(len(word_chunks))\n",
    "    for i, wc in enumerate(word_chunks):\n",
    "        for ngram in example_ngrams_list:\n",
    "            if any([ngram in w for w in wc]):\n",
    "                word_chunks_contain_example_ngrams[i] = 1\n",
    "                break\n",
    "\n",
    "    # get resp curves\n",
    "    rows = stories_data_dict[\"rows\"][story_num]\n",
    "    voxel_nums = rows[\"voxel_num\"]\n",
    "    expls = rows[\"expl\"]\n",
    "    r_curves = resps_dict[\n",
    "        stories_data_dict[\"story_name_new\"][story_num].replace('_resps', '')\n",
    "    ].T[voxel_nums]\n",
    "\n",
    "    for voxel_num in range(17):\n",
    "        expl_voxel = expls[voxel_num]\n",
    "        voxel_resp = r_curves[voxel_num]\n",
    "        word_chunk_deltas.append(\n",
    "            sasc.analyze_helper.compute_word_chunk_deltas_for_single_paragraph(\n",
    "                start_times,\n",
    "                end_times,\n",
    "                voxel_resp,\n",
    "                word_chunks_contain_example_ngrams,\n",
    "                voxel_num,\n",
    "            ))\n",
    "        sasc.viz.plot_annotated_resp(\n",
    "            voxel_num,\n",
    "            word_chunks,\n",
    "            voxel_resp,\n",
    "            expl_voxel,\n",
    "            start_times,\n",
    "            end_times,\n",
    "            stories_data_dict,\n",
    "            expls,\n",
    "            story_num,\n",
    "            word_chunks_contain_example_ngrams,\n",
    "            annotate_texts=False,\n",
    "            plot_key_ngrams=False,\n",
    "        )\n",
    "    analyze_helper.save_figs_to_single_pdf(\n",
    "        filename=join(sasc.config.RESULTS_DIR,\n",
    "                      'figs/curves',\n",
    "                      f'{stories_data_dict[\"story_name_new\"][story_num][3:-10]}_curves.pdf')\n",
    "    )\n",
    "    plt.close(\"all\")\n",
    "\n",
    "\n",
    "# Plot ngram delta\n",
    "means = []\n",
    "sems = []\n",
    "delta_offsets = [1, 2, 3, 4, 5]\n",
    "deltas_3 = []\n",
    "for i in delta_offsets:\n",
    "    deltas = sum([x[i] for x in word_chunk_deltas], [])\n",
    "    if i == 3:\n",
    "        deltas_3 = deltas\n",
    "    # plt.hist(deltas, label='Delta ' + str(i), alpha=0.5)\n",
    "    # plt.axvline(np.mean(deltas), linewidth=1, color=f'C{i - 1}')\n",
    "\n",
    "    means.append(np.mean(deltas))\n",
    "    sems.append(np.std(deltas) / np.sqrt(len(deltas)))\n",
    "    print(f'Delta {i} mean: {means[-1]:0.4f} +/- {sems[-1]:0.4f}')\n",
    "# plt.legend()\n",
    "delta_offsets = 2 * np.array(delta_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008567587727716427"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conduct t-test on whether mean of deltas > 0\n",
    "from scipy.stats import ttest_1samp\n",
    "t, p = ttest_1samp(deltas_3, 0, alternative='greater')\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barplot of means\n",
    "# sns.\n",
    "# plt.grid(zorder=-100)\n",
    "# plt.bar(delta_offsets, means, color='#9dc9e0', zorder=100, width=1.5, alpha=1)\n",
    "# , width=1.5, alpha=1)\n",
    "plt.plot(delta_offsets, means, zorder=100, color='black')\n",
    "plt.errorbar(delta_offsets, means, yerr=sems,\n",
    "             fmt='o', color='black', zorder=101, capsize=4)\n",
    "plt.xticks(delta_offsets)\n",
    "\n",
    "plt.xlabel('Seconds after presentation of key ngram')\n",
    "plt.ylabel('Voxel response change ($\\sigma$)')\n",
    "plt.savefig(join(sasc.config.RESULTS_DIR, 'figs', 'misc',\n",
    "            'pilot_default_ngram_deltas.pdf'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown by explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas_by_expl = pd.DataFrame({\n",
    "    'expl': np.tile(expls.values, 6),\n",
    "    'deltas': [x[3] for x in word_chunk_deltas]\n",
    "})\n",
    "# groupby expl and concatenate list values\n",
    "deltas_by_expl = deltas_by_expl.groupby(\n",
    "    'expl').agg(lambda x: sum(x, [])).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas_by_expl['delta_mean'] = deltas_by_expl['deltas'].apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas_by_expl[['expl', 'delta_mean']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9ff692d44ea03fd8a03facee7621117bbbb82def09bacaacf0a2cbc238b7b91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
